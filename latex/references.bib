
@article{wang2024survey,
  title={A Survey on Large Language Model based Autonomous Agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  journal={Frontiers of Computer Science},
  year={2024}
}

@article{driess2023palme,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Fu, Quan and Murthy, Mose and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{jiang2024crom,
    title={CROM: A Comprehensive Review of Multi-modal Foundation Models for Earth Observation},
    author={Jiang, Zhitong and and others},
    year={2024},
    journal={arXiv preprint arXiv:2402.06432}
}

@inproceedings{savva2019habitat,
  title={Habitat: A platform for embodied ai research},
  author={Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9339--9347},
  year={2019}
}

@inproceedings{anderson2018vln,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and van den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3674--3683},
  year={2018}
}

@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Niessner, Matthias},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5828--5839},
  year={2017}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@article{jakubik2023prithvi,
    title={Prithvi: A Foundational Model for Earth Observation},
    author={Jakubik, Johannes and and others},
    year={2023},
    journal={IBM Research}
}

@article{urbanplanning2025,
    title={Urban planning in the age of large language models: Assessing OpenAI o1's performance and capabilities across 556 tasks},
    author={Unknown},
    journal={Computers, Environment and Urban Systems},
    year={2025}
}

@article{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{das2018eqa,
    title={Embodied Question Answering},
    author={Das, Abhishek and and others},
    year={2018},
    journal={arXiv preprint arXiv:1711.11543}
}

@article{ahn2022saycan,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@inproceedings{yao2021alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Yao, Shunyu and Yu, Dian and Sclar, Matthew and Talamadupula, Kartik and Narasimhan, Karthik},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{srivastava2021behavior,
    title={BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments},
    author={Srivastava, Sanjana and and others},
    booktitle={Conference on Robot Learning},
    year={2021}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C Lawrence and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@article{liu2023agentbench,
  title={Agentbench: Evaluating llms as agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hantao and Xu, Yimeng and Xu, Yifei and Zhang, Jiao and Zhang, Ruoqi and Li, Yujia and Wang, Peiyu and Zheng, Tian and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2023}
}

@article{yang2025embodiedbench,
    title={EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents},
    author={Yang, Rui and and others},
    year={2025},
    journal={arXiv preprint arXiv:2502.09560}
}

@article{thompson2025rem,
    title={REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories},
    author={Thompson, Jacob and Garcia-Lopez, Emiliano and Bisk, Yonatan},
    year={2025},
    journal={arXiv preprint arXiv:2512.00736}
}
