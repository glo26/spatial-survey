\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[margin=1in]{geometry}

\begin{document}

% Title with horizontal rules (matching Attention Is All You Need)
\begin{center}
\rule{\textwidth}{1.5pt}
\vspace{0.3cm}

{\LARGE \bf Autonomous Spatial Intelligence: A Survey of Agentic AI Methods and Evaluation}

\vspace{0.3cm}
\rule{\textwidth}{0.5pt}
\vspace{0.8cm}

% Authors in grid layout (matching Attention Is All You Need style)
\begin{tabular}{ccc}
\textbf{Gloria Felicia} & \textbf{Nolan Bryant} & \textbf{Handi Putra} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
gloria.felicia@atlaspro.ai & nolan.bryant@atlaspro.ai & handi.putra@atlaspro.ai \\
\end{tabular}

\vspace{0.5cm}

\begin{tabular}{ccc}
\textbf{Ayaan Gazali} & \textbf{Eliel Lobo} & \textbf{Esteban Rojas} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
ayaan.gazali@atlaspro.ai & eliel.lobo@atlaspro.ai & esteban.rojas@atlaspro.ai \\
\end{tabular}

\vspace{1cm}

% Abstract
{\large \bf Abstract}
\end{center}

\vspace{0.3cm}

\begin{quote}
The convergence of Agentic Artificial Intelligence and Spatial Intelligence marks a pivotal frontier in the pursuit of creating machines that can autonomously operate in the physical world. While agentic systems demonstrate increasingly sophisticated capabilities in planning and tool use, their ability to perceive, reason about, and interact with complex spatial environments remains a significant bottleneck. This survey addresses a critical gap in the existing literature by providing a unified taxonomy that systematically connects the architectural components of agentic AI with the functional requirements of spatial intelligence. We review the foundational concepts of agentic systems, including memory, planning, and tool use, and categorize the diverse landscape of spatial tasks, including navigation, scene understanding, manipulation, and large-scale geospatial analysis. Through a comprehensive analysis of state-of-the-art methods, including embodied agents, multimodal large language models, and geometric graph neural networks, we evaluate the current capabilities and limitations of these systems. We further analyze the fragmented landscape of evaluation benchmarks, highlighting the urgent need for more integrated and holistic frameworks. By synthesizing these disparate research areas and outlining a forward-looking research roadmap, this paper aims to accelerate the development of robust, safe, and effective spatially-aware autonomous systems.
\end{quote}

\section{Introduction}

The evolution of Artificial Intelligence is marked by a paradigm shift from specialized models to goal-oriented, self-directed agents capable of complex decision-making in dynamic environments. This field, which we term \textbf{Agentic AI}, represents a significant leap towards creating machines that can operate with a higher degree of autonomy. Concurrently, the ability for these agents to perceive, comprehend, and act within the physical world, a capability we define as \textbf{Spatial Intelligence}, has become a primary bottleneck and a critical area of research. The convergence of these two domains is essential for developing AI systems that can effectively and safely navigate real-world complexities, from autonomous vehicles and robotic assistants to large-scale urban planning and disaster response systems.

Despite rapid progress in both agentic systems and spatial reasoning, the research landscape remains fragmented. Numerous surveys have independently covered topics such as Large Language Model agents \citep{yao2023react, wang2024survey, huang2024understanding, xi2023rise, guo2024large}, embodied AI \citep{wang2023voyager, driess2023palme, xiao2023robot}, and geospatial analysis \citep{janowicz2025geofm, mai2024opportunities}. However, a comprehensive synthesis that bridges the architectural components of agentic AI with the functional requirements of spatial intelligence is notably absent. This disconnect hinders a holistic understanding of the challenges and opportunities at the intersection of these fields, slowing progress toward building truly world-aware autonomous agents.

This survey aims to fill this critical gap. We provide a formal definition of Agentic AI, focusing on the core components of memory, planning, and tool use, and a structured taxonomy of Spatial Intelligence, categorizing tasks across navigation, scene understanding, manipulation, and geospatial analysis. Our primary contributions are threefold:

\begin{enumerate}
    \item A novel, unified taxonomy that connects agentic architectures with spatial intelligence tasks, providing a structured framework for understanding and categorizing research in this interdisciplinary area.
    \item A comprehensive review of the state-of-the-art methods, evaluation benchmarks, and real-world applications, synthesizing findings from over 500 papers.
    \item A forward-looking analysis of the open challenges and a research roadmap to guide future work in developing more capable, robust, and safe spatially-aware agentic systems.
\end{enumerate}

By providing this synthesis, we aim to create a foundational reference for researchers, developers, and policymakers, fostering a more integrated approach to building the next generation of autonomous intelligence.

\section{A Taxonomy of Spatial Intelligence}

We define \textbf{Spatial Intelligence} as an agent's ability to perceive, reason about, and interact with the physical world. We propose a taxonomy that categorizes spatial tasks into four key domains:

\textbf{Navigation.} The ability to plan and execute paths in a physical environment. This includes tasks like point-to-point navigation \citep{savva2019habitat, shah2023vint}, vision-language navigation \citep{anderson2018vln, chen2019touchdown, hong2020vlnbert, shah2023lmnav}, and exploration \citep{wang2023voyager, zhou2023esc}.

\textbf{Scene Understanding.} The ability to perceive and reason about the objects, relationships, and context of a 3D scene. This includes tasks like 3D object detection \citep{dai2017scannet, peng2023openscene}, semantic segmentation \citep{dai2017scannet, takmaz2023openmask3d}, and spatial relationship understanding \citep{johnson2017clevr, suhr2019nlvr2, hudson2019gqa, armeni2019scene, wald2020learning, conceptgraphs2024}.

\textbf{Manipulation.} The ability to interact with and modify objects in the environment. This includes tasks like object rearrangement \citep{lin2022vima, wu2023tidybot}, tool use \citep{schick2023toolformer, liang2023code}, and assembly \citep{okamura2000overview}.

\textbf{Geospatial Analysis.} The ability to reason about and analyze large-scale geographic data. This includes tasks like land use classification \citep{sumbul2019bigearthnet, janowicz2025geofm}, change detection \citep{zhang2018siamesecnn}, and urban planning \citep{zheng2014urban, jin2023stgnn}.

\section{Core Components and Architectures of Agentic AI}

Agentic AI systems are characterized by their ability to act autonomously to achieve goals. We identify three core components that enable this autonomy, drawing from the unified framework proposed by \citet{wang2024survey} and \citet{sumers2024cognitive}:

\textbf{Memory.} The ability to store and retrieve information from past experiences. This includes short-term memory for in-context learning \citep{brown2020gpt3} and long-term memory for retaining knowledge and skills, as demonstrated in Generative Agents \citep{park2023generative} and agents with mapping memory \citep{gupta2019neuralslam, huang2023vlmaps}.

\textbf{Planning.} The ability to decompose a high-level goal into a sequence of executable actions. This includes techniques like chain-of-thought reasoning \citep{wei2022chain}, the more deliberate tree-of-thought search \citep{yao2023tree}, and hierarchical planning \citep{song2023llmplanner, zhang2023graph, lin2023text2motion}.

\textbf{Tool Use.} The ability to leverage external tools to extend the agent's capabilities. This includes using APIs for information retrieval \citep{schick2023toolformer, lewis2020rag}, invoking specialized models for specific tasks \citep{accarino2022mrkl}, and interacting with physical actuators \citep{brohan2022rt1, brohan2023rt2}.

\subsection{Agentic Architectures}

Several prominent architectures have emerged to orchestrate these components:

\textbf{ReAct (Reason+Act).} This architecture \citep{yao2023react} interleaves reasoning traces with actions, allowing the agent to create, maintain, and adjust plans while interacting with an external environment. The reasoning traces enable the model to handle exceptions, and update its plan based on the outcomes of its actions.

\textbf{Reflexion.} This framework \citep{shinn2023reflexion} enhances agents with dynamic memory and self-reflection capabilities. After a task failure, the agent reflects on the feedback to identify the cause of the error and updates its internal memory to avoid repeating the same mistake in subsequent trials.

\textbf{Tree of Thoughts (ToT).} ToT \citep{yao2023tree} generalizes over chain-of-thought by exploring multiple reasoning paths in a tree structure. This allows the agent to deliberately explore different lines of reasoning, self-evaluate its progress, and backtrack when necessary, making it more suitable for complex planning tasks.

\textbf{Multi-Agent Systems.} A growing area of research focuses on the collaboration of multiple agents to solve complex problems. Frameworks like AutoGen \citep{wu2023autogen}, MetaGPT \citep{hong2024metagpt}, and CAMEL \citep{li2023camel} enable sophisticated multi-agent conversations and workflows.

\section{State-of-the-Art Methods and Industry Agents}

\subsection{Embodied AI and Vision-Language-Action (VLA) Models}

Embodied AI focuses on creating agents that can learn and act in physical or simulated environments. A key development in this area is the rise of Vision-Language-Action (VLA) models, which are trained to map multimodal inputs (vision, language) directly to robotic actions. These models are at the forefront of creating general-purpose robots.

\textbf{RT-2 (Robotics Transformer 2).} Developed by Google DeepMind, RT-2 \citep{brohan2023rt2} is a VLA model that leverages large-scale web data to learn general concepts about the world and transfer them to robot control. It demonstrates emergent capabilities, such as reasoning about novel objects and executing tasks it was not explicitly trained on.

\textbf{PaLM-E.} This 562-billion parameter embodied multimodal language model from Google \citep{driess2023palme} integrates continuous sensor data from robotic systems directly into a large language model. This allows it to ground language in real-world perception and perform a variety of robotic tasks without task-specific training.

\textbf{Octo and OpenVLA.} Recent open-source efforts like Octo \citep{octo2024} and OpenVLA \citep{kim2024openvla} are democratizing access to powerful VLA models, enabling broader research and development in the community.

\subsection{Industry Applications of Spatial AI}

\textbf{Geospatial Intelligence (Palantir, ESRI).}
Companies like Palantir and ESRI provide powerful platforms for geospatial intelligence. Palantir's Gotham and Foundry platforms are used by government agencies and commercial enterprises for data fusion, pattern detection, and predictive analysis on large-scale geospatial data \citep{palantir2024gotham}. ESRI's ArcGIS platform integrates GeoAI capabilities, enabling users to apply machine learning and deep learning to spatial data for applications in urban planning, environmental monitoring, and business analytics \citep{esri2024geoai}.

\textbf{Location-Based Services (Foursquare, Google Maps).}
Foursquare has evolved from a social check-in app to a leading location intelligence platform, providing rich data on places and human mobility \citep{foursquare2024places}. Google Maps leverages AI for navigation, route optimization, and local search, with recent advancements in conversational and accessible navigation powered by large language models \citep{google2024mapsai}.

\textbf{Autonomous Vehicles (Waymo, Tesla).}
Autonomous driving is a major application of spatial AI. Companies like Waymo and Tesla are developing sophisticated systems that use a combination of sensors, high-definition maps, and AI to perceive the environment, predict the behavior of other agents, and plan safe and efficient routes \citep{waymo2024safety, tesla2024autopilot}. The development of large-scale datasets like nuScenes \citep{caesar2020nuscenes} and Waymo Open Dataset \citep{sun2020waymoopen} has been crucial for advancing research in this area.

\textbf{Smart Cities and Urban Computing.}
Spatial AI is being used to address a wide range of challenges in urban environments. This includes traffic forecasting \citep{li2018dcrnn, yu2018stgcn}, optimizing public transportation systems, and managing urban infrastructure. The field of urban computing leverages large-scale urban data to improve the quality of life for city dwellers \citep{zheng2014urban}.

\textbf{Disaster Response and Emergency Management.}
AI-powered analysis of satellite and aerial imagery is being used to assess damage after natural disasters, identify areas in need of assistance, and coordinate response efforts. Datasets like xBD \citep{gupta2019creating} have been developed to train models for building damage assessment.

\section{A Brief Overview of Benchmarks}

A comprehensive analysis of benchmarks for spatial AI agents will be the subject of a follow-up paper. However, a brief overview is necessary to contextualize the current state of evaluation. Existing benchmarks can be broadly categorized into:

\begin{itemize}
    \item \textbf{Navigation Benchmarks:} R2R \citep{anderson2018vln}, Habitat \citep{savva2019habitat}, ZSON \citep{majumdar2022zson}, CoWs on Pasture \citep{gadre2023cows}
    \item \textbf{Manipulation Benchmarks:} ALFWorld \citep{yao2021alfworld}, BEHAVIOR \citep{srivastava2021behavior}, TidyBot \citep{wu2023tidybot}
    \item \textbf{Spatial Reasoning Benchmarks:} CLEVR \citep{johnson2017clevr}, GQA \citep{hudson2019gqa}, Open3DVQA \citep{zhang2025open3dvqa}
    \item \textbf{Integrated Agent Benchmarks:} AgentBench \citep{liu2023agentbench}, EmbodiedBench \citep{yang2025embodiedbench}, WebArena \citep{zhou2023webarena}
\end{itemize}

These benchmarks, while valuable, often focus on narrow aspects of spatial intelligence. A key challenge for the community is the development of more holistic evaluation frameworks that can assess the full range of agentic capabilities in complex, open-ended spatial tasks.

\section{Open Challenges and Future Directions}

Despite significant progress, several key challenges remain:

\textbf{Robust Spatial Representation.} Developing representations that capture the complexity of 3D environments and generalize across different scenes \citep{mildenhall2020nerf, dai2017scannet, chang2017matterport3d, kerr2023lerf, rosinol2020kimera, hughes2022hydra}.

\textbf{Hierarchical Planning.} Creating agents that can plan over long horizons and decompose complex spatial tasks into manageable sub-goals \citep{song2023llmplanner, zhang2023graph, hao2023rap, huang2023instruct2act}.

\textbf{Safe and Reliable Tool Use.} Ensuring that agents can use tools safely and effectively, especially in safety-critical applications, as highlighted by the SafeAgentBench benchmark \citep{safeagentbench2025} and research on constitutional AI \citep{bai2022constitutional}.

\textbf{Sim-to-Real Transfer.} Bridging the gap between simulation and the real world to enable the deployment of embodied agents in real-world applications \citep{savva2019habitat, shen2021igibson, zhao2020sim}.

\textbf{World Models.} A promising direction is the development of world models that can learn a comprehensive understanding of the world and predict future states \citep{feng2025worldmodels, ding2024worldmodels, zhen20243dvla}. These models could enable more robust and generalizable agents.

\section{Conclusion}

This survey has provided a comprehensive overview of the intersection of Agentic AI and Spatial Intelligence. We have proposed a unified taxonomy, reviewed the state-of-the-art, and identified key challenges and future directions. We believe that by fostering a more integrated approach to research in this area, we can accelerate the development of truly intelligent autonomous systems that can understand and interact with the physical world.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
