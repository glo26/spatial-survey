\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[margin=1in]{geometry}

\begin{document}

% Title with horizontal rules (matching Attention Is All You Need)
\begin{center}
\rule{\textwidth}{1.5pt}
\vspace{0.3cm}

{\LARGE \bf Autonomous Spatial Intelligence: A Survey of Agentic AI Methods and Evaluation}

\vspace{0.3cm}
\rule{\textwidth}{0.5pt}
\vspace{0.8cm}

% Authors in grid layout (matching Attention Is All You Need style)
\begin{tabular}{ccc}
\textbf{Gloria Felicia} & \textbf{Handi Putra} & \textbf{Ayaan Gazali} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
gloria.felicia@atlaspro.ai & handi.putra@atlaspro.ai & ayaan.gazali@atlaspro.ai \\
\end{tabular}

\vspace{0.5cm}

\begin{tabular}{cc}
\textbf{Eliel Lobo} & \textbf{Esteban Rojas} \\
AtlasPro AI & AtlasPro AI \\
eliel.lobo@atlaspro.ai & esteban.rojas@atlaspro.ai \\
\end{tabular}

\vspace{1cm}

% Abstract
{\large \bf Abstract}
\end{center}

\vspace{0.3cm}

\begin{quote}
The convergence of Agentic Artificial Intelligence and Spatial Intelligence marks a pivotal frontier in the pursuit of creating machines that can autonomously operate in the physical world. While agentic systems demonstrate increasingly sophisticated capabilities in planning and tool use, their ability to perceive, reason about, and interact with complex spatial environments remains a significant bottleneck. This survey addresses a critical gap in the existing literature by providing a unified taxonomy that systematically connects the architectural components of agentic AI with the functional requirements of spatial intelligence. We review the foundational concepts of agentic systems, including memory, planning, and tool use, and categorize the diverse landscape of spatial tasks, including navigation, scene understanding, manipulation, and large-scale geospatial analysis. Through a comprehensive analysis of state-of-the-art methods, including embodied agents, multimodal large language models, and geometric graph neural networks, we evaluate the current capabilities and limitations of these systems. We further analyze the fragmented landscape of evaluation benchmarks, highlighting the urgent need for more integrated and holistic frameworks. By synthesizing these disparate research areas and outlining a forward-looking research roadmap, this paper aims to accelerate the development of robust, safe, and effective spatially-aware autonomous systems.
\end{quote}

\section{Introduction}

The evolution of Artificial Intelligence is marked by a paradigm shift from specialized models to goal-oriented, self-directed agents capable of complex decision-making in dynamic environments. This field, which we term \textbf{Agentic AI}, represents a significant leap towards creating machines that can operate with a higher degree of autonomy. Concurrently, the ability for these agents to perceive, comprehend, and act within the physical world, a capability we define as \textbf{Spatial Intelligence}, has become a primary bottleneck and a critical area of research. The convergence of these two domains is essential for developing AI systems that can effectively and safely navigate real-world complexities, from autonomous vehicles and robotic assistants to large-scale urban planning and disaster response systems.

Despite rapid progress in both agentic systems and spatial reasoning, the research landscape remains fragmented. Numerous surveys have independently covered topics such as Large Language Model agents \citep{yao2023react, wang2024survey, huang2024understanding}, embodied AI \citep{wang2023voyager, driess2023palme}, and geospatial analysis \citep{jakubik2024prithvi, cong2022satmae, manas2021seco}. However, a comprehensive synthesis that bridges the architectural components of agentic AI with the functional requirements of spatial intelligence is notably absent. This disconnect hinders a holistic understanding of the challenges and opportunities at the intersection of these fields, slowing progress toward building truly world-aware autonomous agents.

This survey aims to fill this critical gap. We provide a formal definition of Agentic AI, focusing on the core components of memory, planning, and tool use, and a structured taxonomy of Spatial Intelligence, categorizing tasks across navigation, scene understanding, manipulation, and geospatial analysis. Our primary contributions are threefold:

\begin{enumerate}
    \item A novel, unified taxonomy that connects agentic architectures with spatial intelligence tasks, providing a structured framework for understanding and categorizing research in this interdisciplinary area.
    \item A comprehensive review of the state-of-the-art methods, evaluation benchmarks, and real-world applications, synthesizing findings from previously disparate fields.
    \item A forward-looking analysis of the open challenges and a research roadmap to guide future work in developing more capable, robust, and safe spatially-aware agentic systems.
\end{enumerate}

By providing this synthesis, we aim to create a foundational reference for researchers, developers, and policymakers, fostering a more integrated approach to building the next generation of autonomous intelligence.

\section{A Taxonomy of Spatial Intelligence}

We define \textbf{Spatial Intelligence} as an agent's ability to perceive, reason about, and interact with the physical world. We propose a taxonomy that categorizes spatial tasks into four key domains:

\textbf{Navigation.} The ability to plan and execute paths in a physical environment. This includes tasks like point-to-point navigation \citep{savva2019habitat}, vision-language navigation \citep{anderson2018vln, chen2019touchdown, hong2020vlnbert}, and exploration \citep{wang2023voyager}.

\textbf{Scene Understanding.} The ability to perceive and reason about the objects, relationships, and context of a 3D scene. This includes tasks like 3D object detection \citep{dai2017scannet}, semantic segmentation \citep{dai2017scannet}, and spatial relationship understanding \citep{johnson2017clevr, suhr2019nlvr2, hudson2019gqa}.

\textbf{Manipulation.} The ability to interact with and modify objects in the environment. This includes tasks like object rearrangement \citep{lin2022vima}, tool use \citep{schick2023toolformer}, and assembly.

\textbf{Geospatial Analysis.} The ability to reason about and analyze large-scale geographic data. This includes tasks like land use classification \citep{sumbul2019bigearthnet}, change detection \citep{zhang2018siamesecnn}, and urban planning \citep{zheng2014urban}.

\section{Core Components and Architectures of Agentic AI}

Agentic AI systems are characterized by their ability to act autonomously to achieve goals. We identify three core components that enable this autonomy, drawing from the unified framework proposed by \citet{wang2024survey}:

\textbf{Memory.} The ability to store and retrieve information from past experiences. This includes short-term memory for in-context learning and long-term memory for retaining knowledge and skills, as demonstrated in Generative Agents \citep{park2023generative} and agents with mapping memory \citep{gupta2019neuralslam}.

\textbf{Planning.} The ability to decompose a high-level goal into a sequence of executable actions. This includes techniques like chain-of-thought reasoning \citep{wei2022chain}, the more deliberate tree-of-thought search \citep{yao2023tree}, and hierarchical planning \citep{song2023llmplanner, zhang2023graph}.

\textbf{Tool Use.} The ability to leverage external tools to extend the agent's capabilities. This includes using APIs for information retrieval \citep{schick2023toolformer, lewis2020rag}, invoking specialized models for specific tasks \citep{accarino2022mrkl}, and interacting with physical actuators.

\subsection{Agentic Architectures}

Several prominent architectures have emerged to orchestrate these components:

\textbf{ReAct (Reason+Act).} This architecture \citep{yao2023react} interleaves reasoning traces with actions, allowing the agent to create, maintain, and adjust plans while interacting with an external environment. The reasoning traces enable the model to handle exceptions, and update its plan based on the outcomes of its actions.

\textbf{Reflexion.} This framework \citep{shinn2023reflexion} enhances agents with dynamic memory and self-reflection capabilities. After a task failure, the agent reflects on the feedback to identify the cause of the error and updates its internal memory to avoid repeating the same mistake in subsequent trials.

\textbf{Tree of Thoughts (ToT).} ToT \citep{yao2023tree} generalizes over chain-of-thought by exploring multiple reasoning paths in a tree structure. This allows the agent to deliberately explore different lines of reasoning, self-evaluate its progress, and backtrack when necessary, making it more suitable for complex planning tasks.

\section{State-of-the-Art Methods and Industry Agents}

\subsection{Embodied AI and Vision-Language-Action (VLA) Models}

Embodied AI focuses on creating agents that can learn and act in physical or simulated environments. A key development in this area is the rise of Vision-Language-Action (VLA) models, which are trained to map multimodal inputs (vision, language) directly to robotic actions. These models are at the forefront of creating general-purpose robots.

\textbf{RT-2 (Robotics Transformer 2).} Developed by Google DeepMind, RT-2 \citep{brohan2023rt2} is a VLA model that leverages large-scale web data to learn general concepts about the world and transfer them to robot control. It demonstrates emergent capabilities, such as reasoning about novel objects and executing tasks it was not explicitly trained on.

\textbf{PaLM-E.} This 562-billion parameter embodied multimodal language model from Google \citep{driess2023palme} integrates continuous sensor data from robotic systems directly into a large language model. This allows it to ground language in real-world perception and perform a variety of robotic tasks without task-specific training.

\subsection{Industry Agents for Spatial Planning}

Several notable agents from industry showcase the application of these architectures to complex spatial planning tasks:

\textbf{Voyager.} Developed by NVIDIA, Voyager \citep{wang2023voyager} is an LLM-powered embodied agent that excels at open-ended exploration and skill acquisition in Minecraft. It uses GPT-4 to generate a curriculum, write code for new skills, and store them in a skill library for long-term use.

\textbf{SayCan.} This Google project \citep{ahn2022saycan} grounds language models in robotic affordances. It uses an LLM to determine high-level actions and a learned value function to assess the feasibility of those actions for a given robot, effectively bridging the gap between abstract reasoning and physical capability.

\textbf{Code as Policies.} This approach \citep{liang2023code} uses LLMs to generate Python code that serves as a reactive policy for a robot. This allows for more complex and dynamic behaviors than direct action prediction and leverages the extensive knowledge of programming and logic embedded in LLMs.

\section{A Brief Overview of Benchmarks}

A comprehensive analysis of benchmarks for spatial AI agents will be the subject of a follow-up paper. However, a brief overview is necessary to contextualize the current state of evaluation. Existing benchmarks can be broadly categorized into:

\begin{itemize}
    \item \textbf{Navigation Benchmarks:} R2R \citep{anderson2018vln}, Habitat \citep{savva2019habitat}
    \item \textbf{Manipulation Benchmarks:} ALFWorld \citep{yao2021alfworld}, BEHAVIOR \citep{srivastava2021behavior}
    \item \textbf{Spatial Reasoning Benchmarks:} CLEVR \citep{johnson2017clevr}, GQA \citep{hudson2019gqa}
    \item \textbf{Integrated Agent Benchmarks:} AgentBench \citep{liu2023agentbench}, EmbodiedBench \citep{yang2025embodiedbench}
\end{itemize}

These benchmarks, while valuable, often focus on narrow aspects of spatial intelligence. A key challenge for the community is the development of more holistic evaluation frameworks that can assess the full range of agentic capabilities in complex, open-ended spatial tasks.

\section{Open Challenges and Future Directions}

Despite significant progress, several key challenges remain:

\textbf{Robust Spatial Representation.} Developing representations that capture the complexity of 3D environments and generalize across different scenes \citep{mildenhall2020nerf, dai2017scannet, chang2017matterport3d}.

\textbf{Hierarchical Planning.} Creating agents that can plan over long horizons and decompose complex spatial tasks into manageable sub-goals \citep{song2023llmplanner, zhang2023graph, hao2023rap}.

\textbf{Safe and Reliable Tool Use.} Ensuring that agents can use tools safely and effectively, especially in safety-critical applications, as highlighted by the SafeAgentBench benchmark \citep{safeagentbench2025} and research on constitutional AI \citep{bai2022constitutional}.

\textbf{Sim-to-Real Transfer.} Bridging the gap between simulation and the real world to enable the deployment of embodied agents in real-world applications \citep{savva2019habitat, shen2021igibson}.

\section{Conclusion}

This survey has provided a comprehensive overview of the intersection of Agentic AI and Spatial Intelligence. We have proposed a unified taxonomy, reviewed the state-of-the-art, and identified key challenges and future directions. We believe that by fostering a more integrated approach to research in this area, we can accelerate the development of truly intelligent autonomous systems that can understand and interact with the physical world.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
