\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tcolorbox}

% Define colors for takeaway boxes
\definecolor{takeawayblue}{RGB}{230,240,250}

\begin{document}

% Title with horizontal rules (matching Attention Is All You Need)
\begin{center}
\rule{\textwidth}{1.5pt}
\vspace{0.3cm}

{\LARGE \bf Autonomous Spatial Intelligence: A Comprehensive Survey of\\Agentic AI Methods for Physical World Understanding}

\vspace{0.3cm}
\rule{\textwidth}{0.5pt}
\vspace{0.8cm}

% Authors in grid layout
\begin{tabular}{ccc}
\textbf{Gloria Felicia} & \textbf{Nolan Bryant} & \textbf{Handi Putra} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
gloria.felicia@atlaspro.ai & nolan.bryant@atlaspro.ai & handi.putra@atlaspro.ai \\
\end{tabular}

\vspace{0.5cm}

\begin{tabular}{ccc}
\textbf{Ayaan Gazali} & \textbf{Eliel Lobo} & \textbf{Esteban Rojas} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
ayaan.gazali@atlaspro.ai & eliel.lobo@atlaspro.ai & esteban.rojas@atlaspro.ai \\
\end{tabular}

\vspace{1cm}

{\large \bf Abstract}
\end{center}

\vspace{0.3cm}

\begin{quote}
The dominant approaches for creating autonomous agents are based on large language models, which excel at reasoning and planning \citep{brown2020gpt3, openai2023gpt4, touvron2023llama2, team2023gemini, anthropic2024claude, dubey2024llama3, achiam2023gpt4, anil2023palm}. However, these models lack the innate spatial intelligence required to perceive, navigate, and interact with the complex physical world, a critical gap for embodied AI \citep{chen2024spatialvlm, yang2025embodiedbench, duan2022surveyembodiedaisimulators, amin2024embodied, cheng2025embodiedeval}. We introduce a unified three-axis taxonomy that systematically connects agentic AI architectures with spatial intelligence capabilities across spatial scales, providing the first comprehensive framework for this convergent domain. We synthesize over 900 papers, revealing three key findings: (1) hierarchical memory systems are critical for long-horizon spatial tasks \citep{packer2023memgpt, banino2018vector, xu2025amemagenticmemoryllm, zhang2025memevolvemetaevolutionagentmemory}; (2) GNN-LLM integration is an emergent paradigm for structured spatial reasoning \citep{jin2023stgnn, chen2024llaga, chen2024graphgpt, chai2023graphllm}; and (3) world models are essential for safe deployment in physical environments \citep{hafner2023dreamerv3, bruce2024genie, ha2018worldmodels, feng2025worldmodels, ding2024worldmodels, brooks2024sora}. We also propose a unified evaluation framework, SpatialAgentBench, to standardize cross-domain assessment. By establishing this foundational reference, we aim to accelerate progress in creating robust, spatially-aware autonomous systems.
\end{quote}

\section{Introduction}

The pursuit of artificial general intelligence increasingly centers on creating agents that can perceive, reason about, and act within physical environments \citep{turing1950computing, brooks1991intelligence, russell2010artificial, lecun2015deep, goodfellow2016deep, bengio2013representation, laird2019soar}. While large language models have demonstrated remarkable capabilities in reasoning and planning \citep{brown2020gpt3, openai2023gpt4, wei2022chain, touvron2023llama2, team2023gemini}, their ability to operate effectively in spatial contexts remains a fundamental challenge \citep{chen2024spatialvlm, yang2025embodiedbench, cheng2024spatialrgpt}.

The emergence of multimodal foundation models has accelerated progress in visual understanding \citep{radford2021clip, liu2023llava, li2023blip2, alayrac2022flamingo, openai2023gpt4v, dosovitskiy2021vit, chen2023pali, chen2024pali3, chen2024internvl, bai2023qwenvl, dai2023instructblip}, yet translating this understanding into effective spatial action remains challenging. The gap between language-based reasoning and physical world interaction represents one of the most significant obstacles to achieving truly capable autonomous systems \citep{ahn2022saycan, brohan2023rt2, driess2023palme, liang2023code}.

We define \textbf{Agentic AI} as systems exhibiting goal-directed behavior through autonomous decision-making, characterized by four core capabilities: persistent memory for experience accumulation, planning for action sequencing, tool use for capability extension, and self-reflection for continuous improvement \citep{wang2024survey, xi2023rise, yao2023react, shinn2023reflexion, park2023generative, wu2023autogen, hong2023metagpt, durante2024agent, guo2024large}. These agents operate through iterative cycles of perception, reasoning, action, and feedback, enabling complex task completion in dynamic environments \citep{yao2023react, shinn2023reflexion, madaan2023selfrefine}.

Complementarily, \textbf{Spatial Intelligence} encompasses the ability to perceive 3D structure, reason about object relationships, navigate environments, and manipulate physical objects \citep{chen2024spatialvlm, marr1982vision, newcombe2010spatial, chen2024spatialreasoning, cai2024spatialbot, cheng2024spatialrgpt}. This includes understanding geometric relationships, predicting physical dynamics, and planning actions that account for spatial constraints \citep{battaglia2018relational, scarselli2009graph, gilmer2017neural, kipf2017semi, velivckovic2018gat, hamilton2017inductive}.

The convergence of these domains is essential for real-world AI applications across multiple sectors. Autonomous vehicles must perceive dynamic environments and plan safe trajectories \citep{hu2023uniad, caesar2020nuscenes, waymo2023, geiger2012kitti, cadena2016slam, chen2024endtoend, waymo_emma_2024, tian2024drivevlm}. Robotic assistants require understanding of object affordances and spatial relationships \citep{brohan2023rt2, ahn2022saycan, team2024octo, kim2024openvla, driess2023palme, black2024pi0, bharadhwaj2024roboagent, open_x_embodiment_rt_x_2023}. Urban computing systems must model complex spatio-temporal dependencies \citep{jin2023stgnn, li2018dcrnn, yu2018stgcn, wu2019graphwavenet, zheng2014urban, cui2024spatiotemporalinteraction, cini2023taming}. Geospatial intelligence platforms must analyze satellite imagery and geographic data at scale \citep{jakubik2024prithvi, cong2022satmae, mai2023opportunities, janowicz2020geoai, li2025autonomousgis, bastani2023satlaspretrain, esri2024geoai, xiao2025foundationmodelsremotesensing}. Despite this importance, existing surveys treat these areas in isolation, lacking a unified framework connecting agentic architectures with spatial requirements.

\textbf{Contributions.} This survey makes five primary contributions:
\begin{enumerate}[leftmargin=*]
    \item A \textbf{unified three-axis taxonomy} connecting agentic AI components (memory, planning, tool use) with spatial intelligence domains (navigation, scene understanding, manipulation, geospatial analysis) across spatial scales (micro, meso, macro), providing a structured framework for interdisciplinary research.
    \item A \textbf{comprehensive analysis} of over 900 papers identifying key architectural patterns, including the emergence of GNN-LLM integration, vision-language-action models, and world model-based planning as critical enablers for spatial reasoning.
    \item A \textbf{systematic comparison} with existing surveys, demonstrating how this work uniquely bridges agentic AI and spatial intelligence domains.
    \item The \textbf{proposal of a unified evaluation framework, SpatialAgentBench}, with 8 tasks spanning navigation, manipulation, scene understanding, and geospatial reasoning to standardize cross-domain assessment.
    \item A \textbf{forward-looking roadmap} identifying grand challenges and research directions for developing robust, safe, and capable spatially-aware autonomous systems.
\end{enumerate}

\section{Methodology}

This survey follows a systematic literature review methodology consistent with best practices in computer science \citep{kitchenham2004procedures, petersen2008systematic, wohlin2014guidelines, keele2007guidelines, brereton2007lessons, dyba2007applying, moher2009preferred}. We queried major academic databases including Google Scholar, arXiv, ACM Digital Library, IEEE Xplore, Semantic Scholar, and DBLP \citep{ley2002dblp} with keywords including ``agentic AI,'' ``spatial intelligence,'' ``embodied AI,'' ``vision-language navigation,'' ``robot manipulation,'' ``geospatial AI,'' ``world models,'' ``graph neural networks,'' ``spatio-temporal learning,'' ``vision-language-action,'' and ``foundation models for robotics.'' Our initial search yielded over 3,000 papers.

We then applied a rigorous multi-stage filtering process:

\begin{enumerate}
    \item \textbf{Temporal Filtering:} We selected papers published between 2018 and 2026, with emphasis on recent advances while including foundational works that established key paradigms.
    \item \textbf{Venue Filtering:} We prioritized papers from top-tier venues including NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, CoRL, RSS, IROS, ICRA, ACM Computing Surveys, IEEE TPAMI, Nature, Science, Science Robotics, and leading arXiv preprints.
    \item \textbf{Quality Filtering:} We prioritized papers with high citation counts, those representing foundational methods, and state-of-the-art contributions that advance the field.
    \item \textbf{Relevance Filtering:} We ensured papers directly addressed the intersection of agentic capabilities and spatial intelligence.
\end{enumerate}

This process resulted in a final corpus of over 900 papers, which were systematically analyzed to derive the taxonomy, identify key trends, and synthesize the findings presented in this survey. We employed a snowball sampling technique to ensure comprehensive coverage of related works, following citation chains both forward and backward. Two independent reviewers validated the paper selection and taxonomy development.

\section{Related Work and Survey Comparison}

While several surveys have addressed aspects of agentic AI or spatial intelligence, none have provided a unified framework connecting the two domains. We review existing surveys across five categories and provide a systematic comparison in Table~\ref{tab:survey_comparison}.

\textbf{Agentic AI Surveys.} Recent surveys on LLM-based agents \citep{wang2024survey, xi2023rise, guo2024large} focus on reasoning and tool use but do not address spatial capabilities. \citet{sumers2024cognitive} provides a cognitive architecture perspective. \citet{chan2023chateval} evaluates conversational agents.

\textbf{Embodied AI Surveys.} Embodied AI surveys \citep{duan2022surveyembodiedaisimulators, gupta2021embodied, francis2022corl} cover simulation environments and benchmarks but lack integration with agentic architectures. \citet{kawaharazuka2025vla} surveys vision-language-action models specifically for robotics.

\textbf{Geospatial AI Surveys.} Geospatial AI surveys \citep{mai2023opportunities, janowicz2020geoai, xiao2025foundationmodelsremotesensing} and spatio-temporal data mining reviews \citep{jin2023stgnn, atluri2018spatiotemporal, wang2020deep} are highly specialized and do not connect to general agentic systems. \citet{zhan2024neuralnetworksgeospatialdata} surveys neural networks for geospatial data.

\textbf{Graph Neural Network Surveys.} GNN surveys \citep{wu2020gnnsurvey, bronstein2021geometric, hamilton2020graph, battaglia2018relational} provide comprehensive coverage of graph learning but do not focus on spatial applications or agent integration. Surveys on GNNs for specific domains include traffic \citep{jiang2022graph}, urban computing \citep{balachandar2025urbanincidentpredictiongraph}, and spatio-temporal prediction \citep{jin2023stgnn}.

\textbf{Vision-Language Model Surveys.} Surveys on VLMs \citep{zhang2024lmms, bordes2024introduction} cover multimodal understanding but do not address spatial action or embodiment. \citet{kawaharazuka2025vla} surveys vision-language-action models specifically for robotics.

\begin{table}[h!]
\centering
\caption{Comparison with Existing Surveys. Our work is the first to provide unified coverage across all dimensions.}
\label{tab:survey_comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Survey} & \textbf{Agentic AI} & \textbf{Embodied AI} & \textbf{Spatial Reasoning} & \textbf{Geospatial} & \textbf{GNNs} & \textbf{Industry} & \textbf{Unified Taxonomy} \\
\midrule
Wang et al. (2024) \citep{wang2024survey} & \checkmark & $\circ$ & $\circ$ & & & & \\
Xi et al. (2023) \citep{xi2023rise} & \checkmark & $\circ$ & & & & & \\
Duan et al. (2022) \citep{duan2022surveyembodiedaisimulators} & & \checkmark & $\circ$ & & & & \\
Kawaharazuka et al. (2025) \citep{kawaharazuka2025vla} & $\circ$ & \checkmark & $\circ$ & & & & \\
Jin et al. (2023) \citep{jin2023stgnn} & & & $\circ$ & $\circ$ & \checkmark & & \\
Mai et al. (2023) \citep{mai2023opportunities} & & & & \checkmark & & $\circ$ & \\
Bronstein et al. (2021) \citep{bronstein2021geometric} & & & $\circ$ & & \checkmark & & \\
Zhang et al. (2024) \citep{zhang2024lmms} & $\circ$ & & $\circ$ & & & & \\
\midrule
\textbf{This Survey} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\footnotesize{\checkmark = comprehensive coverage, $\circ$ = partial coverage, blank = not covered}
\end{table}

Our work is the first to bridge these gaps, providing a comprehensive, structured analysis of the convergent domain of autonomous spatial intelligence with a unified three-axis taxonomy.

\section{Unified Three-Axis Taxonomy}

We propose a three-axis taxonomy (Figure~\ref{fig:taxonomy}) that maps agentic capabilities to spatial task requirements across spatial scales, enabling systematic analysis of existing methods and identification of research gaps.

\subsection{Taxonomy Axes}

\textbf{Axis 1: Spatial Task.} We identify four primary spatial task categories:
\begin{itemize}[leftmargin=*]
    \item \textbf{Navigation}: Goal-directed movement through environments, including point-goal \citep{anderson2018on, wijmans2019dd}, object-goal \citep{chaplot2020objectgoal, batra2020objectnav}, and vision-language navigation \citep{anderson2018vln, krantz2020vlnce}
    \item \textbf{Scene Understanding}: Perceiving and reasoning about 3D structure, objects, and spatial relationships
    \item \textbf{Manipulation}: Physical interaction with objects, including grasping \citep{mahler2017dexnet, morrison2018closing, fang2020graspnet}, placement \citep{zeng2021transporter}, and tool use \citep{qin2023toolllm, qin2024toolllm}
    \item \textbf{Geospatial Analysis}: Large-scale spatial reasoning including satellite imagery \citep{christie2018fmow, christie2018functional, demir2018deepglobe}, urban computing \citep{zheng2014urban, yuan2020survey}, and geographic information systems \citep{longley2015gis, goodchild2007citizens}
\end{itemize}

\textbf{Axis 2: Agentic Capability.} We identify three core agentic capabilities:
\begin{itemize}[leftmargin=*]
    \item \textbf{Memory}: Short-term (in-context), long-term (retrieval-augmented), episodic, and spatial memory systems
    \item \textbf{Planning}: Reactive, hierarchical, search-based, and world model-based planning approaches
    \item \textbf{Tool Use \& Action}: API integration, code generation, physical action primitives, and skill libraries
\end{itemize}

\textbf{Axis 3: Spatial Scale.} We distinguish three spatial scales:
\begin{itemize}[leftmargin=*]
    \item \textbf{Micro-spatial}: Pose estimation, grasping, fine manipulation (centimeter scale)
    \item \textbf{Meso-spatial}: Room navigation, building exploration, indoor scenes (meter scale)
    \item \textbf{Macro-spatial}: City-scale planning, satellite imagery, infrastructure networks (kilometer scale)
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{taxonomy.png}
    \caption{A unified three-axis taxonomy connecting Agentic AI capabilities (memory, planning, tool use) with Spatial Intelligence domains (navigation, scene understanding, manipulation, geospatial analysis) across spatial scales (micro, meso, macro). The intersection of these dimensions defines the design space for autonomous spatial intelligence systems.}
    \label{fig:taxonomy}
\end{figure}

\subsection{Methods-Taxonomy Mapping}

Table~\ref{tab:methods_mapping} maps representative methods to our three-axis taxonomy, demonstrating how the framework organizes the field.

\begin{table}[h!]
\centering
\caption{Representative Methods Mapped to the Three-Axis Taxonomy}
\label{tab:methods_mapping}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Spatial Task} & \textbf{Agentic Capability} & \textbf{Spatial Scale} & \textbf{Representation} \\
\midrule
VLN-BERT \citep{hong2021vln} & Navigation & Memory + Planning & Meso & Language + Visual \\
SayCan \citep{ahn2022saycan} & Manipulation & Planning + Tool Use & Micro & Language + Affordance \\
RT-2 \citep{brohan2023rt2} & Manipulation & Tool Use & Micro & Visual + Action \\
VLMaps \citep{huang2023vlmaps} & Navigation & Memory & Meso & Semantic Map \\
Voyager \citep{wang2023voyager} & Navigation + Manipulation & Memory + Planning & Meso & Language + Code \\
DCRNN \citep{li2018dcrnn} & Geospatial & Memory & Macro & Graph \\
Graph WaveNet \citep{wu2019graphwavenet} & Geospatial & Memory & Macro & Adaptive Graph \\
Prithvi \citep{jakubik2024prithvi} & Geospatial & - & Macro & Visual (Satellite) \\
DreamerV3 \citep{hafner2023dreamerv3} & Navigation + Manipulation & Planning (World Model) & Meso & Latent \\
PaLM-E \citep{driess2023palme} & Manipulation & Planning + Tool Use & Micro-Meso & Multimodal \\
OpenVLA \citep{kim2024openvla} & Manipulation & Tool Use & Micro & Visual + Action \\
LLaGA \citep{chen2024llaga} & Scene Understanding & Memory & Meso & Graph + Language \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Taxonomy]
\begin{itemize}[leftmargin=*,nosep]
    \item The three-axis taxonomy (Task $\times$ Capability $\times$ Scale) provides a comprehensive framework for organizing spatial AI research
    \item Most methods address meso-spatial scales; micro and macro scales remain underexplored
    \item Memory systems are critical across all spatial tasks but implementations vary significantly by scale
    \item The intersection of GNN-based methods with agentic capabilities represents an emerging frontier
\end{itemize}
\end{tcolorbox}

\section{Agentic AI Components for Spatial Intelligence}

This section examines how agentic capabilities enable spatial intelligence, organized around the core scientific question: \textit{How do agents internally represent, reason about, and act within spatial environments?}

\subsection{Memory Systems: How Do Agents Remember Spatial Information?}

Memory enables agents to accumulate and retrieve experiential knowledge, forming the foundation for learning and adaptation \citep{tulving1972episodic, baddeley2003working, squire2004memory}. The central challenge is: \textit{How can agents maintain persistent spatial knowledge across varying time horizons and scales?}

\textbf{Short-Term Memory.} In-context learning \citep{brown2020gpt3, dong2022survey, olsson2022context, akyurek2023learning, dai2023gpt} allows models to adapt to new tasks through examples in the prompt. This mechanism enables rapid adaptation without parameter updates, leveraging the attention mechanism to condition on provided demonstrations. Working memory mechanisms \citep{graves2014neural, weston2015memory, sukhbaatar2015end, kumar2016ask, santoro2016meta} enable temporary information storage during reasoning, supporting multi-step computations that exceed single forward pass capabilities.

\textbf{Long-Term Memory.} Retrieval-augmented generation \citep{lewis2020rag, packer2023memgpt, guu2020retrieval, borgeaud2022improving, asai2023selfrag, trivedi2023interleaving} enables knowledge persistence beyond context limits. MemGPT \citep{packer2023memgpt} introduces hierarchical memory management for extended conversations. AMEM \citep{xu2025amemagenticmemoryllm} provides agentic memory for LLMs. MemEvolve \citep{zhang2025memevolvemetaevolutionagentmemory} enables meta-evolution of agent memory. Vector databases \citep{johnson2019billion, malkov2018efficient, douze2024faiss, milvus2021, pinecone2023} provide efficient similarity search for memory retrieval, enabling agents to access relevant past experiences.

\textbf{Episodic Memory.} Episodic memory stores specific experiences and events, enabling agents to learn from past interactions \citep{blundell2016model, pritzel2017neural, banino2018vector, ritter2018been, fortunato2019generalization}. This type of memory is critical for spatial agents that must remember visited locations, encountered objects, and successful action sequences \citep{savinov2018episodic, chaplot2020neural, fang2019scene}.

\textbf{Spatial Memory.} Specialized memory for spatial information includes cognitive maps \citep{tolman1948cognitive, okeefe1978hippocampus}, topological representations \citep{kuipers2000spatial, choset2001topological}, and metric maps \citep{thrun2005probabilistic, durrant2006simultaneous, cadena2016slam}. Neural approaches to spatial memory include Neural SLAM \citep{chaplot2020neural, chaplot2020learning}, semantic maps \citep{huang2023vlmaps, henriques2018mapnet, shah2023lmnav}, and scene graphs \citep{armeni2019scene, rosinol2020kimera, hughes2022hydra, gu2024conceptgraphs}.

\textbf{Spatial Failure Modes.} Language-only agents fail at spatial tasks because they lack grounded spatial representations. Key failure modes include: (1) \textit{spatial hallucination}, where agents describe impossible spatial configurations; (2) \textit{reference frame confusion}, where agents conflate egocentric and allocentric coordinates; (3) \textit{scale insensitivity}, where agents fail to distinguish micro, meso, and macro-scale reasoning; and (4) \textit{temporal drift}, where spatial memory degrades over long horizons without explicit persistence mechanisms.

\subsection{Planning Systems: How Do Agents Plan Over Spatial Horizons?}

Planning decomposes goals into executable action sequences, enabling complex task completion \citep{russell2010artificial, ghallab2004automated, lavalle2006planning}. The central challenge is: \textit{How can agents decompose spatial goals into feasible action sequences while accounting for geometric constraints?}

\textbf{Chain-of-Thought Reasoning.} Step-by-step reasoning \citep{wei2022chain, kojima2022large, wang2022self, zhou2023leasttomost, fu2023complexitybased, chen2023program} enables systematic problem decomposition. Self-consistency \citep{wang2022self} improves reliability through multiple reasoning paths. Zero-shot chain-of-thought \citep{kojima2022large} enables reasoning without demonstrations.

\textbf{Tree-Based Search.} Tree of Thoughts \citep{yao2023tree, long2023large} explores multiple solution branches through deliberate search. Graph of Thoughts \citep{besta2023graph, lei2023boosting} enables more complex reasoning structures with arbitrary connections. RAP \citep{hao2023rap, shridhar2020alfworld, zhao2024expel} combines reasoning with acting in a planning framework. Monte Carlo Tree Search variants \citep{silver2016mastering, schrittwieser2020mastering, browne2012survey, anthony2017thinking, silver2017mastering} provide principled exploration with theoretical guarantees.

\textbf{Hierarchical Planning.} LLM-Planner \citep{song2023llmplanner} enables few-shot grounded planning for embodied agents. Inner Monologue \citep{huang2022inner} provides feedback-driven planning through internal dialogue. HiPlan \citep{li2025hiplanhierarchicalplanningllmbased} introduces hierarchical planning with LLMs. Hierarchical RL approaches \citep{nachum2018data, vezhnevets2017feudal, bacon2017option, pertsch2021accelerating, kulkarni2016hierarchical, gupta2019relay} decompose tasks into subtasks with temporal abstraction.

\textbf{Task and Motion Planning.} TAMP \citep{garrett2021integrated, kaelbling2011hierarchical, toussaint2015logic, dantam2016incremental, srivastava2014combined} integrates symbolic planning with continuous motion planning for robotic applications. This approach combines the expressiveness of symbolic reasoning with the precision of geometric planning.

\textbf{LLM-Based Planning.} Recent work leverages LLMs directly for planning \citep{huang2022language, valmeekam2023large, song2023llmplanner, silver2024generalized, liu2023llm+}. SayCan \citep{ahn2022saycan} grounds language models in affordances. Code as Policies \citep{liang2023code} generates executable robot code. ProgPrompt \citep{singh2023progprompt} uses programmatic prompting for task planning.

\textbf{Spatial Planning Failure Modes.} LLM-based planners fail when: (1) \textit{geometric constraints are violated}, producing plans that ignore collision or reachability; (2) \textit{action preconditions are unmet}, sequencing actions without verifying feasibility; (3) \textit{long-horizon credit assignment fails}, losing track of subgoal dependencies; and (4) \textit{dynamic replanning is absent}, failing to adapt when execution diverges from expectations.

\subsection{Tool Use and Action: How Do Agents Ground Language in Geometry?}

Tool use extends agent capabilities through external interfaces and physical actions \citep{osiurak2016tool, vaesen2012cognitive}. The central challenge is: \textit{How can language-based reasoning be translated into precise geometric actions?}

\textbf{API Integration.} Toolformer \citep{schick2023toolformer, parisi2022talm} enables self-supervised tool learning. Gorilla \citep{patil2023gorilla, li2023apibank} specializes in API calling with retrieval augmentation. ToolLLM \citep{qin2023toolllm, hao2024toolkengpt} provides comprehensive tool use benchmarks. TaskMatrix \citep{liang2023taskmatrix, lu2023chameleon} connects foundation models with millions of APIs. TALM \citep{parisi2022talm} augments language models with tool use. Additional tool-use frameworks include HuggingGPT \citep{shen2023hugginggpt}, ViperGPT \citep{suris2023vipergpt}, and Visual ChatGPT \citep{wu2023visual}.

\textbf{Code Generation.} PAL \citep{gao2023pal} uses code for reasoning. Code as Policies \citep{liang2023code} generates executable robot code from language. Codex \citep{chen2021evaluating}, StarCoder \citep{li2023starcoder}, CodeLlama \citep{roziere2023codellama}, and DeepSeek-Coder \citep{guo2024deepseek} provide code generation capabilities. ProgPrompt \citep{singh2023progprompt} uses programmatic prompting for robotics. Self-debugging \citep{chen2023teaching} improves code quality through iterative refinement.

\textbf{ReAct Architecture.} ReAct \citep{yao2023react, yao2023reactsynergizingreasoningacting} interleaves reasoning with action execution, enabling agents to think before acting. Reflexion \citep{shinn2023reflexion, shinn2023reflexionlanguageagentsverbal} adds self-reflection for improvement through verbal reinforcement. These architectures form the foundation for many spatial agents.

\textbf{Physical Action.} For embodied agents, tool use extends to physical manipulation \citep{zeng2021transporter, brohan2022rt1, brohan2023rt2, shridhar2022cliport}. Action primitives \citep{dalal2021accelerating, nasiriany2022augmenting} provide reusable building blocks. Skill libraries \citep{wang2023voyager, lynch2020learning, pertsch2021accelerating} enable compositional action.

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Agentic Components]
\begin{itemize}[leftmargin=*,nosep]
    \item Memory systems must be explicitly spatial: cognitive maps, semantic maps, and scene graphs outperform generic retrieval for spatial tasks
    \item Hierarchical planning with geometric grounding addresses the gap between high-level language goals and low-level motor commands
    \item Tool use bridges language and action through code generation, API calls, and learned action primitives
    \item Key failure modes stem from lack of spatial grounding: hallucination, reference frame confusion, and geometric constraint violation
\end{itemize}
\end{tcolorbox}

\section{Spatial Intelligence Domains}

This section examines the four primary spatial task domains, organized around the question: \textit{What spatial capabilities must agents possess to operate in the physical world?}

\subsection{Navigation: How Do Agents Move Through Space?}

Navigation requires agents to perceive environments, plan paths, and execute locomotion toward goals \citep{bonin2008visual, desouza2002vision, thrun2002robotic}.

\textbf{Vision-Language Navigation.} VLN tasks require agents to follow natural language instructions in visual environments \citep{anderson2018vln, qi2020reverie, krantz2020vlnce, fried2018speaker, chen2022duet, shah2023lmnav, hong2020vlnbert, chen2021hamt, an2023bevbert}. R2R \citep{anderson2018vln} introduced the paradigm. REVERIE \citep{qi2020reverie} adds object grounding. VLN-CE \citep{krantz2020vlnce} extends to continuous environments.

\textbf{Object-Goal Navigation.} ObjectNav requires navigating to object categories \citep{batra2020objectnav, chaplot2020object, majumdar2022zson, gadre2022cow, gadre2023cows, dorbala2022clipnav}. ZSON \citep{majumdar2022zson} enables zero-shot navigation. CLIP-Nav \citep{dorbala2022clipnav} leverages vision-language models. CoW \citep{gadre2022cow} uses CLIP on wheels for semantic navigation.

\textbf{Audio-Visual Navigation.} Audio cues guide navigation in SoundSpaces \citep{chen2020soundspaces, chen2022soundspaces2, gan2020look}. This modality is critical for finding sound-emitting targets.

\textbf{Embodied Question Answering.} EQA requires navigation to answer questions \citep{das2018embodied, gordon2018iqa, wijmans2019embodied, yu2019multi}. 3D-QA \citep{azuma2022scanqa, ma2022sqa3d, hong20233dllm, chen2024ll3da} extends to 3D scene understanding.

\subsection{Scene Understanding: How Do Agents Perceive 3D Structure?}

Scene understanding encompasses perceiving 3D geometry \citep{hartley2003multiple, szeliski2022computer}, recognizing objects \citep{krizhevsky2012imagenet, he2016deep}, and reasoning about spatial relationships \citep{johnson2015image, krishna2017visual}.

\textbf{Neural Scene Representations.} NeRF \citep{mildenhall2020nerf, barron2022mipnerf360, muller2022instant, park2019deepsdf, mescheder2019occupancy, barron2023zipnerf} revolutionized 3D reconstruction. Mip-NeRF \citep{barron2022mipnerf360} handles multi-scale rendering. 3D Gaussian Splatting \citep{kerbl20233dgaussian, luiten2023dynamic, fan2024lightgaussian} enables real-time rendering. Integration with SLAM \citep{sucar2021imap, zhu2022nice, keetha2024splatam, bird2025dvmslam} enables online reconstruction.

\textbf{Point Cloud Processing.} Point cloud methods \citep{qi2017pointnet, qi2017pointnet++, wang2019dgcnn, thomas2019kpconv, zhao2021point} process raw 3D data. Point-BERT \citep{yu2022point}, Point-MAE \citep{pang2022masked}, PointGPT \citep{chen2024pointgpt}, and Point-Bind \citep{guo2023pointbind} introduce self-supervised pretraining. 3D object detection \citep{shi2019pointrcnn, qi2019deep, shi2020pv, chen2023voxelnext} enables scene parsing.

\textbf{Depth Estimation.} Monocular depth estimation \citep{godard2019monodepth2, ranftl2021dpt, ranftl2020midas, yang2024depth, fu2024geowizard} provides geometric understanding from single images. Depth Anything \citep{yang2024depth} achieves strong zero-shot transfer. Metric3D \citep{yin2023metric3d} recovers metric depth.

\textbf{Semantic Segmentation.} Semantic segmentation \citep{long2015fully, chen2017deeplab, kirillov2023segment, peng2023openscene, chen2023clip2scene} enables scene parsing. SAM \citep{kirillov2023segment} provides promptable segmentation. Open-vocabulary methods \citep{ghiasi2022scaling, liang2023open, chen2023open} enable zero-shot recognition.

\subsection{Manipulation: How Do Agents Interact with Objects?}

Manipulation requires understanding object affordances \citep{gibson1979ecological}, planning contact-rich interactions \citep{chitnis2020efficient}, and executing precise motor commands \citep{argall2009survey}.

\textbf{Vision-Language-Action Models.} VLA models \citep{brohan2022rt1, brohan2023rt2, team2024octo, kim2024openvla, black2024pi0, driess2023palme, open_x_embodiment_rt_x_2023} directly map visual observations and language instructions to actions. RT-1 \citep{brohan2022rt1} introduced large-scale robot learning. RT-2 \citep{brohan2023rt2} demonstrated web-scale pretraining transfer. RT-X \citep{open_x_embodiment_rt_x_2023} enables cross-embodiment learning. Octo \citep{team2024octo} provides an open generalist policy. OpenVLA \citep{kim2024openvla} offers open-source VLA. $\pi_0$ \citep{black2024pi0} introduces flow matching for robot learning. RoboCat \citep{bousmalis2023robocat} demonstrates self-improvement.

\textbf{Imitation Learning.} Behavior cloning \citep{pomerleau1988alvinn, chi2023diffusion, zhao2023learning, chi2024diffusion} learns from demonstrations. Diffusion Policy \citep{chi2023diffusion} applies diffusion models to action generation. ACT \citep{zhao2023learning} uses action chunking with transformers. Learning from play \citep{lynch2020learning} enables unstructured learning.

\textbf{Reinforcement Learning.} RL for manipulation \citep{kalashnikov2018qt, levine2018learning, haarnoja2018soft, schulman2017ppo, fujimoto2018td3} enables learning from interaction. QT-Opt \citep{kalashnikov2018qt} scales to real-world grasping. SAC \citep{haarnoja2018soft} provides sample-efficient learning.

\textbf{Simulation Environments.} Simulation platforms \citep{james2020rlbench, yu2020metaworld, makoviychuk2021isaac, savva2019habitat, kolve2017ai2thor, gu2023maniskill2, deitke2023objaverse} provide training environments. RLBench \citep{james2020rlbench} offers diverse manipulation tasks. Meta-World \citep{yu2020metaworld} provides multi-task benchmarks. Isaac Gym \citep{makoviychuk2021isaac} enables GPU-accelerated simulation.

\subsection{Geospatial Analysis: How Do Agents Reason at Planetary Scale?}

Geospatial analysis requires processing satellite imagery \citep{zhu2017deep}, modeling urban dynamics \citep{bibri2017smart}, and reasoning about geographic relationships \citep{egenhofer1991point}.

\textbf{Remote Sensing Foundation Models.} Prithvi \citep{jakubik2024prithvi} provides geospatial foundation models trained on Harmonized Landsat Sentinel-2 data. SatMAE \citep{cong2022satmae} introduces masked autoencoders for satellite imagery. Satlas \citep{bastani2023satlas} enables large-scale geospatial understanding. GeoAI \citep{janowicz2020geoai, mai2023opportunities} surveys the field. CROMA \citep{fuller2024croma} and microestimates \citep{chi2022microestimates} advance remote sensing analysis.

\textbf{Spatio-Temporal Graph Neural Networks.} STGNNs model complex urban dynamics through graph-structured representations \citep{jin2023stgnn, atluri2018spatiotemporal, wang2020deep}. The general formulation combines spatial and temporal operators:
\begin{equation}
\mathbf{H}^{(l+1)} = \sigma\left(\mathbf{A} \mathbf{H}^{(l)} \mathbf{W}^{(l)} + \text{TemporalConv}(\mathbf{H}^{(l)})\right)
\end{equation}

DCRNN \citep{li2018dcrnn} models traffic as diffusion on graphs:
\begin{equation}
\mathbf{H}^{(l)} = \sum_{k=0}^{K} \left(\mathbf{P}_f^k \mathbf{X} \mathbf{W}_{k,1} + \mathbf{P}_b^k \mathbf{X} \mathbf{W}_{k,2}\right)
\end{equation}
where $\mathbf{P}_f$ and $\mathbf{P}_b$ are forward and backward transition matrices.

STGCN \citep{yu2018stgcn} combines graph and temporal convolutions through a sandwiched structure. Graph WaveNet \citep{wu2019graphwavenet} learns adaptive graph structures without predefined adjacency:
\begin{equation}
\tilde{\mathbf{A}} = \text{SoftMax}\left(\text{ReLU}\left(\mathbf{E}_1 \mathbf{E}_2^T\right)\right)
\end{equation}
where $\mathbf{E}_1, \mathbf{E}_2$ are learnable node embeddings.

AGCRN \citep{bai2020agcrn} introduces node-specific patterns through adaptive modules. ASTGCN \citep{guo2019astgcn} adds spatial and temporal attention mechanisms. GMAN \citep{zheng2020gman} uses graph multi-attention with transform attention for long-range dependencies. STGRAT \citep{choi2022stgrat} advances the field.

\textbf{Urban Computing.} Urban computing \citep{zheng2014urban, yuan2020survey} applies AI to city-scale challenges. ST-LLM \citep{liu2024stllm} and UniST \citep{yuan2024unist} integrate language models with spatio-temporal reasoning. Traffic prediction \citep{li2018dcrnn, yu2018stgcn, wu2019graphwavenet} and demand forecasting \citep{geng2019spatiotemporal} represent key applications.

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Spatial Domains]
\begin{itemize}[leftmargin=*,nosep]
    \item Navigation has progressed from point-goal to language-guided and zero-shot paradigms through vision-language integration
    \item Scene understanding benefits from neural implicit representations (NeRF, 3DGS) combined with semantic grounding
    \item Manipulation is being transformed by VLA models that transfer web-scale knowledge to robotic control
    \item Geospatial analysis increasingly leverages foundation models and GNNs for planetary-scale reasoning
\end{itemize}
\end{tcolorbox}

\section{Enabling Technologies}

\subsection{Graph Neural Networks for Spatial Reasoning}

GNNs provide inductive biases well-suited to spatial reasoning through message passing on graph structures \citep{kipf2017semi, velivckovic2018graph, xu2019powerful, hamilton2017inductive, wu2020comprehensive}.

\textbf{Message Passing Framework.} The general GNN formulation follows the message passing paradigm \citep{gilmer2017neural, scarselli2009graph, battaglia2018relational}:
\begin{align}
\mathbf{m}_v^{(l)} &= \text{AGGREGATE}^{(l)}\left(\left\{\mathbf{h}_u^{(l-1)} : u \in \mathcal{N}(v)\right\}\right) \\
\mathbf{h}_v^{(l)} &= \text{UPDATE}^{(l)}\left(\mathbf{h}_v^{(l-1)}, \mathbf{m}_v^{(l)}\right)
\end{align}
where $\mathcal{N}(v)$ denotes the neighbors of node $v$, and AGGREGATE and UPDATE are learnable functions.

\textbf{GNN-LLM Integration.} Emerging work combines GNNs with LLMs for structured spatial reasoning \citep{chen2024llaga, tang2024graphgpt, fatemi2023talk, fatemi2024talk, gowda2025graphs}. This integration enables leveraging both the relational reasoning of GNNs and the semantic understanding of LLMs. Graph instruction tuning \citep{zhang2024graphinstruct} further enhances this capability. LLaGA \citep{chen2024llaga} provides language-graph alignment. GraphGPT \citep{tang2024graphgpt} enables graph reasoning through language models.

\textbf{Geometric Deep Learning.} Geometric deep learning \citep{bronstein2021geometric} provides theoretical foundations for spatial reasoning on non-Euclidean domains. Equivariant networks \citep{cohen2016group, fuchs2020se3, satorras2021en} respect spatial symmetries through:
\begin{equation}
f(T_g \cdot x) = T_g \cdot f(x)
\end{equation}
where $T_g$ is a group transformation. Graph transformers \citep{ying2021graphormer, dwivedi2023benchmarking, rampasek2022gps} combine attention with graph structure. E3NN \citep{batzner2022e3nn} and geometric message passing \citep{brandstetter2022geometric} advance equivariant architectures.

\subsection{World Models}

World models learn predictive representations enabling planning through imagination \citep{lecun2022path, schmidhuber2015learning, matsuo2022deep}.

\textbf{Latent Dynamics Models.} World models learn a latent dynamics model that predicts future states:
\begin{align}
\text{Encoder:} \quad & \mathbf{z}_t = q_\phi(\mathbf{z}_t | \mathbf{o}_{\leq t}, \mathbf{a}_{<t}) \\
\text{Dynamics:} \quad & \hat{\mathbf{z}}_{t+1} = p_\theta(\hat{\mathbf{z}}_{t+1} | \mathbf{z}_t, \mathbf{a}_t) \\
\text{Decoder:} \quad & \hat{\mathbf{o}}_t = p_\psi(\hat{\mathbf{o}}_t | \mathbf{z}_t)
\end{align}

\textbf{Model-Based Reinforcement Learning.} Dreamer \citep{hafner2019dreamer} introduced latent imagination for sample-efficient learning through recurrent state-space models. DreamerV2 \citep{hafner2021dreamerv2} achieved human-level Atari performance with discrete latent states. DreamerV3 \citep{hafner2023dreamerv3} demonstrated cross-domain mastery with a single algorithm through symlog predictions. DayDreamer \citep{wu2023daydreamer} transferred world models to real robots with minimal real-world data. PlaNet \citep{hafner2019learning} pioneered latent dynamics learning. MuZero \citep{schrittwieser2020mastering} combined learned models with MCTS for game playing. Additional approaches include MBPO \citep{janner2019mbpo}, World Models \citep{ha2018worldmodels}, and TD-MPC \citep{hansen2022tdmpc}.

\textbf{Video World Models.} Genie \citep{bruce2024genie} learns controllable world models from internet videos enabling interactive environments. WorldDreamer \citep{yang2024worlddreamer} generates driving world models for autonomous vehicles. GAIA-1 \citep{hu2023gaia1} produces realistic driving videos conditioned on actions and text. Sora \citep{openai2024sora} demonstrates video generation as world simulation at scale. Video prediction models \citep{yang2024video, baker2022video} provide foundations for world understanding.

\textbf{LLM-Based World Models.} LLMs can serve as world models for planning \citep{hao2023rap, huang2022language}, predicting state transitions without explicit environment models. This approach leverages the vast knowledge encoded in LLMs to simulate world dynamics. RAP \citep{hao2023rap} combines reasoning with acting through world model rollouts. TransDreamer \citep{chen2022transdreamer} and UniSim \citep{yang2023unisim} advance world modeling.

\subsection{Multimodal Foundation Models}

Multimodal models integrate vision, language, and action understanding \citep{baltruvsaitis2019multimodal, xu2023multimodal}.

\textbf{Vision-Language Models.} CLIP \citep{radford2021clip} enabled zero-shot visual recognition through contrastive pretraining on web-scale data. BLIP-2 \citep{li2023blip2} introduced efficient vision-language pretraining with frozen encoders. LLaVA \citep{liu2023llava, liu2024llavanext} demonstrated visual instruction tuning with strong performance. GPT-4V \citep{openai2023gpt4v, zheng2024gpt4vision, yan2023gpt4v} achieved strong multimodal reasoning. Gemini \citep{team2023gemini} provides native multimodal capabilities. Flamingo \citep{alayrac2022flamingo} enables few-shot visual learning through interleaved attention. PaLI \citep{chen2022pali, chen2023pali} scales vision-language models. Kosmos-2 \citep{peng2023kosmos2} adds grounding capabilities. Qwen-VL \citep{bai2023qwenvl} provides open multilingual VLMs. Additional models include InstructBLIP \citep{dai2023instructblip}, CogVLM \citep{wang2023cogvlm}, InternVL \citep{chen2024internvl2}, and Ferret \citep{you2023ferret}.

\textbf{Spatial Vision-Language Models.} SpatialVLM \citep{chen2024spatialvlm} specializes in spatial reasoning with fine-grained understanding. SpatialRGPT \citep{cheng2024spatialrgpt} provides regional spatial reasoning. VoxPoser \citep{huang2023voxposer} extracts affordances from VLMs into 3D representations. VLMaps \citep{huang2023vlmaps} creates semantic spatial maps for navigation. These models bridge vision-language understanding with spatial reasoning.

\textbf{3D Vision-Language Models.} 3D-LLM \citep{hong20233dllm} enables language understanding of 3D scenes. Open3D-VQA \citep{zhang2025open3dvqa} provides open-vocabulary 3D visual question answering. LLM-Grounder \citep{yang2024llmgrounder} grounds language in 3D environments.

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Enabling Technologies]
\begin{itemize}[leftmargin=*,nosep]
    \item GNN-LLM integration represents a paradigm shift, combining relational reasoning with semantic understanding
    \item World models enable sample-efficient learning and safe planning through imagination
    \item Spatial VLMs (SpatialVLM, VLMaps, VoxPoser) bridge the gap between vision-language understanding and spatial action
    \item Equivariant architectures provide principled approaches to geometric reasoning
\end{itemize}
\end{tcolorbox}

\section{Industry Applications as Design Patterns}

Rather than cataloging company capabilities, we abstract industry deployments into reusable design patterns for spatial AI systems.

\subsection{Design Pattern 1: Human-in-the-Loop Spatial Reasoning}

This pattern combines AI spatial analysis with human expert validation \citep{amershi2019guidelines, shneiderman2022human}, exemplified by:

\textbf{Geospatial Intelligence.} Palantir \citep{palantir2023} integrates AI with human analysts for defense and commercial applications. The Gotham platform enables intelligence analysis with spatial reasoning while maintaining human oversight for critical decisions.

\textbf{GIS Workflows.} ESRI \citep{esri2023, esri2024geoai} provides ArcGIS with integrated GeoAI capabilities where AI assists human planners in urban planning, environmental monitoring, and disaster response. The pattern: AI proposes, human validates, system learns from corrections.

\subsection{Design Pattern 2: Weakly Supervised Planetary-Scale Learning}

This pattern leverages massive unlabeled data with minimal supervision for global-scale models \citep{ratner2017snorkel, zhang2022survey}:

\textbf{Satellite Foundation Models.} NASA-IBM Prithvi \citep{jakubik2024prithvi} trains on Harmonized Landsat Sentinel-2 data using self-supervised learning. Planet Labs \citep{planet_labs_2023} operates the largest Earth-imaging constellation, enabling daily global monitoring. Maxar provides high-resolution imagery for defense applications. The pattern: self-supervised pretraining on petabyte-scale imagery, fine-tuning for specific tasks.

\textbf{Mapping at Scale.} Google \citep{googlemaps2023, google2024mapsai} deploys AI for global-scale mapping through Google Earth Engine and Maps AI. The pattern: leverage user interactions and multi-source data for continuous model improvement.

\subsection{Design Pattern 3: Agent-Assisted Spatial Workflows}

This pattern deploys AI agents to augment human spatial reasoning \citep{shneiderman2020human, horvitz1999principles}:

\textbf{Autonomous GIS.} AutonomousGIS \citep{li2025autonomousgis} and GeoGPT \citep{bai2024geogpt} integrate agentic capabilities with geospatial analysis. The pattern: LLM-based agents that can query spatial databases, generate maps, and answer geographic questions.

\textbf{Location Intelligence.} Foursquare \citep{foursquare2023} and Carto \citep{carto2023} provide location-based services with AI-powered analytics. Wherobots \citep{wherobots2023} offers cloud-native spatial analytics. The pattern: spatial data infrastructure with AI-powered query and analysis.

\subsection{Design Pattern 4: Embodied AI at Scale}

This pattern deploys learned spatial policies in physical systems \citep{kober2013reinforcement, levine2016end}:

\textbf{Autonomous Vehicles.} Waymo \citep{waymo2023, waymo_emma_2024} has deployed autonomous vehicles at scale with millions of miles driven. EMMA \citep{waymo_emma_2024} provides end-to-end multimodal models for driving. Tesla \citep{tesla2023fsd} pursues vision-only autonomy. The pattern: massive simulation, careful real-world deployment, continuous learning from fleet data.

\textbf{Robot Learning Platforms.} Open X-Embodiment \citep{open_x_embodiment_rt_x_2023} provides large-scale robot data from Google DeepMind and collaborating institutions. Bridge Data \citep{walke2023bridgedata} enables cross-domain transfer. The pattern: diverse data collection, foundation model training, transfer to specific embodiments.

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Industry Patterns]
\begin{itemize}[leftmargin=*,nosep]
    \item Human-in-the-loop patterns dominate safety-critical applications (defense, urban planning)
    \item Weakly supervised learning enables planetary-scale models without exhaustive labeling
    \item Agent-assisted workflows augment rather than replace human spatial reasoning
    \item Embodied AI deployment requires massive simulation followed by careful real-world transfer
\end{itemize}
\end{tcolorbox}

\section{Evaluation Framework and Benchmark Analysis}

\subsection{Existing Benchmarks}

Table~\ref{tab:benchmarks} summarizes key benchmarks organized by our taxonomy.

\begin{table}[h!]
\centering
\caption{Spatial AI Benchmarks Organized by Taxonomy}
\label{tab:benchmarks}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Benchmark} & \textbf{Spatial Task} & \textbf{Scale} & \textbf{Environment} & \textbf{Primary Metric} & \textbf{Agentic Capability} \\
\midrule
R2R \citep{anderson2018vln} & Navigation & Meso & Simulated & SPL, SR & Memory + Planning \\
REVERIE \citep{qi2020reverie} & Navigation & Meso & Simulated & SPL, RGS & Memory + Planning \\
Habitat \citep{savva2019habitat} & Navigation & Meso & Simulated & SPL & Planning \\
AI2-THOR \citep{kolve2017ai2thor} & Navigation + Manipulation & Meso & Simulated & SR & Planning + Tool Use \\
RLBench \citep{james2020rlbench} & Manipulation & Micro & Simulated & SR & Tool Use \\
Meta-World \citep{yu2020metaworld} & Manipulation & Micro & Simulated & SR & Tool Use \\
nuScenes \citep{caesar2020nuscenes} & Scene Understanding & Meso-Macro & Real & mAP, NDS & Memory \\
KITTI \citep{geiger2012kitti} & Scene Understanding & Meso & Real & mAP & Memory \\
ScanNet \citep{dai2017scannet} & Scene Understanding & Meso & Real & mIoU & Memory \\
AgentBench \citep{liu2023agentbench} & General & - & Mixed & SR & All \\
WebArena \citep{zhou2023webarena} & Web & - & Simulated & SR & Planning + Tool Use \\
SWE-Bench \citep{jimenez2024swebench} & Code & - & Real & Pass@k & Planning + Tool Use \\
EmbodiedBench \citep{yang2025embodiedbench} & Embodied & Meso & Simulated & SR & All \\
SafeAgentBench \citep{safeagentbench2025} & Safety & - & Simulated & Safety Rate & Planning \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Evaluation Metrics}

We propose standardized metrics across domains with formal definitions \citep{powers2011evaluation, sokolova2009systematic, hossin2015review}:

\textbf{Navigation Metrics.} Success Rate (SR) measures task completion. Success weighted by Path Length (SPL) \citep{anderson2018evaluation} accounts for path efficiency:
\begin{equation}
\text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{\ell_i}{\max(\ell_i, p_i)}
\end{equation}
where $S_i$ is the binary success indicator, $\ell_i$ is the shortest path length, and $p_i$ is the actual path length.

Normalized Dynamic Time Warping (nDTW) measures trajectory similarity:
\begin{equation}
\text{nDTW} = \exp\left(-\frac{\text{DTW}(P, R)}{\ell_R}\right)
\end{equation}
where $P$ is the predicted path, $R$ is the reference path, and $\ell_R$ is the reference path length.

\textbf{Manipulation Metrics.} Task Success Rate measures goal achievement. Goal Condition Satisfaction evaluates partial completion. Efficiency metrics include action count and time to completion.

\textbf{Reasoning Metrics.} Accuracy, F1 Score, and BLEU scores assess spatial reasoning and question answering.

\textbf{Safety Metrics.} Collision Rate, Safety Violation Rate, and Risk-Aware Success measure safe operation.

\subsection{Critical Analysis: What Benchmarks Fail to Measure}

While existing benchmarks have advanced the field \citep{raji2021ai, liao2021we, ribeiro2020beyond}, several fundamental limitations warrant critical examination:

\textbf{Simulation-Reality Gap.} Most benchmarks rely on simulated environments \citep{savva2019habitat, kolve2017ai2thor, james2020rlbench, chattopadhyay2021robustnav, szot2021habitat2}, which differ from real-world conditions in visual appearance, physics, and dynamics. Policies trained in simulation often fail to transfer \citep{zhao2020sim, tobin2017domain, andrychowicz2020learning}, limiting practical applicability. \textit{Gap: No benchmark systematically measures sim-to-real transfer degradation.}

\textbf{Metric Limitations.} Standard metrics like SPL assume optimal paths are known, which is unrealistic in novel environments. Success Rate ignores partial progress and efficiency. Current metrics do not capture important aspects such as safety, robustness to perturbations, and graceful degradation. \textit{Gap: Metrics reward task completion but not safe, robust, or interpretable behavior.}

\textbf{Long-Horizon Evaluation.} Most benchmarks evaluate short episodes (tens to hundreds of steps). Real-world tasks require sustained performance over hours or days with memory persistence and error recovery. \textit{Gap: No benchmark evaluates multi-day spatial tasks with persistent memory.}

\textbf{Safety-Critical Evaluation.} Benchmarks rarely evaluate failure modes, adversarial robustness, or behavior under distribution shift. Safety-critical applications require understanding of worst-case performance. \textit{Gap: Safety evaluation remains ad-hoc rather than systematic.}

\textbf{Cross-Scale Evaluation.} Benchmarks typically operate at a single spatial scale. Real applications require reasoning across micro (grasping), meso (navigation), and macro (planning) scales simultaneously. \textit{Gap: No benchmark evaluates cross-scale spatial reasoning.}

\subsection{SpatialAgentBench: Proposed Unified Framework}

Our proposed benchmark addresses these gaps with eight tasks designed to evaluate the full spectrum of spatial agent capabilities:

\begin{enumerate}
    \item \textbf{VLN-Instruct}: Vision-language navigation with complex, multi-step instructions requiring spatial reasoning and landmark recognition.
    \item \textbf{ObjectSearch}: Multi-room object search with semantic reasoning, requiring agents to leverage commonsense knowledge about object locations.
    \item \textbf{SceneQA}: 3D scene question answering requiring understanding of spatial relationships, object properties, and scene semantics.
    \item \textbf{ManipSeq}: Sequential manipulation planning with long-horizon tasks requiring tool use and state tracking.
    \item \textbf{GeoReason}: Geospatial reasoning from satellite imagery including change detection, land use classification, and spatial pattern analysis.
    \item \textbf{TrafficPredict}: Spatio-temporal traffic prediction requiring modeling of complex urban dynamics and graph-structured dependencies.
    \item \textbf{SafeNav}: Navigation with safety constraints including obstacle avoidance, social navigation, and risk-aware planning.
    \item \textbf{MultiAgent}: Coordinated multi-agent spatial tasks requiring communication, task allocation, and collaborative planning.
\end{enumerate}

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Key Takeaways: Evaluation]
\begin{itemize}[leftmargin=*,nosep]
    \item Existing benchmarks are fragmented across domains with incompatible metrics
    \item Critical gaps exist in sim-to-real transfer, long-horizon, safety-critical, and cross-scale evaluation
    \item SpatialAgentBench proposes unified evaluation across navigation, manipulation, scene understanding, and geospatial reasoning
    \item Standardized metrics (SPL, nDTW, safety rates) enable cross-domain comparison
\end{itemize}
\end{tcolorbox}

\section{Grand Challenges and Future Directions}

We identify six grand challenges that represent fundamental bottlenecks for the field \citep{marcus2020next, chollet2019measure, lake2017building, bengio2019system, bommasani2021opportunities}:

\subsection{Grand Challenge 1: Unified Spatial Representation}

\textit{How can agents maintain a single, coherent spatial representation that supports reasoning across micro, meso, and macro scales?}

Current approaches use separate representations for different scales: point clouds for grasping \citep{rusu20113d, fang2020graspnet}, topological maps for navigation \citep{thrun1998learning, kuipers1991robot}, and raster imagery for geospatial analysis \citep{goodfellow2016deep}. A unified representation would enable seamless reasoning across scales. Key research directions include:

\begin{itemize}
    \item Hierarchical scene graphs that span from object parts to city infrastructure
    \item Neural implicit representations with multi-scale querying
    \item Foundation models for 3D understanding \citep{hong20233dllm, fu20243dfm, shen2023point, oquab2024dinov2, oquab2023dinov2, chen2023open, zhou2024uni3d, wu20153d}
\end{itemize}

\subsection{Grand Challenge 2: Grounded Long-Horizon Planning}

\textit{How can agents plan over extended horizons while maintaining geometric feasibility?}

LLMs can generate high-level plans but struggle with geometric constraints \citep{valmeekam2023planning, kambhampati2024llms, huang2024understanding}. TAMP systems handle geometry but lack semantic flexibility. Bridging this gap requires:

\begin{itemize}
    \item Hybrid neuro-symbolic planners that combine LLM reasoning with geometric verification
    \item Hierarchical planning with learned abstractions \citep{song2023llmplanner, valmeekam2023large, huang2022inner, li2025hiplanhierarchicalplanningllmbased, silver2024generalized}
    \item World models that predict both semantic and geometric consequences
\end{itemize}

\subsection{Grand Challenge 3: Safe Deployment Under Uncertainty}

\textit{How can spatial AI systems operate safely in safety-critical applications with guaranteed bounds on failure?}

Current systems lack formal safety guarantees \citep{seshia2022toward, koopman2019safety, amodei2016concrete}. Deployment in autonomous vehicles, medical robotics, and infrastructure requires:

\begin{itemize}
    \item Uncertainty quantification for spatial predictions
    \item Out-of-distribution detection for novel environments
    \item Formal verification of spatial reasoning \citep{safeagentbench2025, amodei2016safety, amodei2016concrete, bai2022constitutional}
    \item Graceful degradation under adversarial conditions
\end{itemize}

\subsection{Grand Challenge 4: Sim-to-Real Transfer}

\textit{How can policies learned in simulation transfer reliably to the physical world?}

The reality gap affects perception, dynamics, and control \citep{peng2018sim, rusu2017sim, sadeghi2017cad2rl}. Bridging requires:

\begin{itemize}
    \item Photorealistic simulation with accurate physics \citep{zhao2020sim, tobin2017domain, james2019sim, matas2018sim}
    \item Domain randomization and adaptation
    \item Real-world fine-tuning with minimal data
    \item Hybrid simulation-real training pipelines
\end{itemize}

\subsection{Grand Challenge 5: Scalable Multi-Agent Coordination}

\textit{How can large numbers of spatial agents coordinate effectively with limited communication?}

Current multi-agent systems scale poorly \citep{stone2000multiagent, busoniu2008comprehensive, foerster2016learning}. Real applications (warehouse robotics, autonomous traffic) require:

\begin{itemize}
    \item Emergent communication protocols for spatial coordination
    \item Decentralized planning with global consistency \citep{zhang2021multi, wu2023autogen, hong2023metagpt, li2023s, qian2023communicative}
    \item Heterogeneous agent coordination
    \item Robust coordination under partial observability
\end{itemize}

\subsection{Grand Challenge 6: Efficient Edge Deployment}

\textit{How can capable spatial AI systems run on resource-constrained platforms?}

Foundation models require significant compute. Edge deployment requires:

\begin{itemize}
    \item Model compression without capability loss \citep{han2016deep, howard2017mobilenets, dehghani2023scaling}
    \item Efficient architectures for spatial reasoning
    \item Hardware-software co-design for spatial AI
    \item Adaptive compute allocation based on task difficulty
\end{itemize}

\begin{tcolorbox}[colback=takeawayblue,colframe=black!50,title=Grand Challenges Summary]
\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Unified Representation}: Single representation spanning micro to macro scales
    \item \textbf{Grounded Planning}: Long-horizon planning with geometric feasibility
    \item \textbf{Safe Deployment}: Formal safety guarantees for critical applications
    \item \textbf{Sim-to-Real}: Reliable transfer from simulation to physical world
    \item \textbf{Multi-Agent}: Scalable coordination with limited communication
    \item \textbf{Edge Deployment}: Capable systems on resource-constrained platforms
\end{enumerate}
\end{tcolorbox}

\section{Limitations}

This survey, while comprehensive, has several limitations:

\begin{itemize}
    \item Our paper selection process, though systematic, may have missed relevant works in adjacent fields or non-English publications.
    \item The proposed taxonomy, while unifying, is one of many possible categorizations and may not capture all nuances of the field.
    \item Our analysis is based on publicly available information and does not include proprietary details from industry labs.
    \item The field is rapidly evolving, and some recent works may not be fully represented.
    \item We focus primarily on English-language publications from major venues.
    \item The proposed SpatialAgentBench is conceptual and requires implementation and validation.
    \item Our analysis of industry applications relies on public information and may not reflect current capabilities.
\end{itemize}

\section{Conclusion}

This survey has provided a unified three-axis taxonomy connecting Agentic AI and Spatial Intelligence across spatial scales, synthesizing over 900 papers across foundational architectures, state-of-the-art methods, industry applications, and evaluation benchmarks. Our analysis reveals three key findings:

\begin{enumerate}
    \item \textbf{Hierarchical memory systems} are critical for long-horizon spatial tasks, enabling agents to accumulate and retrieve spatial knowledge effectively. Advances in retrieval-augmented generation, episodic memory, and spatial memory representations provide foundations for persistent spatial understanding.
    
    \item \textbf{GNN-LLM integration} is an emergent paradigm combining the relational reasoning of graph networks with the semantic understanding of language models. This integration enables structured spatial reasoning that leverages both geometric relationships and semantic knowledge.
    
    \item \textbf{World models} are essential for safe deployment, enabling agents to predict consequences and plan in imagination before acting. Video world models, latent dynamics models, and LLM-based world models provide complementary approaches to predictive understanding.
\end{enumerate}

We have identified six grand challenges that represent fundamental bottlenecks: unified spatial representation, grounded long-horizon planning, safe deployment under uncertainty, sim-to-real transfer, scalable multi-agent coordination, and efficient edge deployment. The convergence of vision-language-action models, graph neural networks, world models, and foundation models provides promising directions for addressing these challenges.

By establishing this foundational reference with a three-axis taxonomy and proposing SpatialAgentBench, we aim to accelerate progress toward capable, robust, and safe spatially-aware autonomous systems that can perceive, reason about, and act within the physical world. The intersection of agentic AI and spatial intelligence represents a critical frontier for artificial intelligence, with profound implications for autonomous vehicles, robotics, urban computing, and geospatial intelligence.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
