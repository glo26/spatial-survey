\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

% Title with horizontal rules (matching Attention Is All You Need)
\begin{center}
\rule{\textwidth}{1.5pt}
\vspace{0.3cm}

{\LARGE \bf Autonomous Spatial Intelligence: A Comprehensive Survey of\\Agentic AI Methods for Physical World Understanding}

\vspace{0.3cm}
\rule{\textwidth}{0.5pt}
\vspace{0.8cm}

% Authors in grid layout
\begin{tabular}{ccc}
\textbf{Gloria Felicia} & \textbf{Nolan Bryant} & \textbf{Handi Putra} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
gloria.felicia@atlaspro.ai & nolan.bryant@atlaspro.ai & handi.putra@atlaspro.ai \\
\end{tabular}

\vspace{0.5cm}

\begin{tabular}{ccc}
\textbf{Ayaan Gazali} & \textbf{Eliel Lobo} & \textbf{Esteban Rojas} \\
AtlasPro AI & AtlasPro AI & AtlasPro AI \\
ayaan.gazali@atlaspro.ai & eliel.lobo@atlaspro.ai & esteban.rojas@atlaspro.ai \\
\end{tabular}

\vspace{1cm}

{\large \bf Abstract}
\end{center}

\vspace{0.3cm}

\begin{quote}
The dominant approaches for creating autonomous agents are based on large language models, which excel at reasoning and planning \citep{brown2020gpt3, openai2023gpt4, touvron2023llama, touvron2023llama2, team2023gemini, anthropic2024claude, dubey2024llama3, yang2024qwen2, achiam2023gpt4}. However, these models lack the innate spatial intelligence required to perceive, navigate, and interact with the complex physical world, a critical gap for embodied AI \citep{chen2024spatialvlm, yang2025embodiedbench, huang2023voxposer, duan2022surveyembodiedaisimulators, gupta2021embodied, amin2024embodied}. We introduce a unified taxonomy that systematically connects agentic AI architectures with spatial intelligence capabilities, providing the first comprehensive framework for this convergent domain. We synthesize over 900 papers, revealing three key findings: (1) hierarchical memory systems are critical for long-horizon spatial tasks \citep{packer2023memgpt, xu2025amemagenticmemoryllm, zhang2025memevolvemetaevolutionagentmemory, banino2018vector}; (2) GNN-LLM integration is an emergent paradigm for structured spatial reasoning \citep{jin2023stgnn, jin2023spatiotemporalgraphneuralnetworks, shehzad2024graphtransformers, chen2024llaga}; and (3) world models are essential for safe deployment in physical environments \citep{hafner2023dreamerv3, bruce2024genie, feng2025worldmodels, ding2024worldmodels, ha2018worldmodels}. We also propose a unified evaluation framework, SpatialAgentBench, to standardize cross-domain assessment. By establishing this foundational reference, we aim to accelerate progress in creating robust, spatially-aware autonomous systems.
\end{quote}

\section{Introduction}

The pursuit of artificial general intelligence increasingly centers on creating agents that can perceive, reason about, and act within physical environments \citep{mccarthy1955proposal, turing1950computing, nilsson1984shakey, moravec1988sensor, brooks1991intelligence, laird2019soar, russell2010artificial, goodfellow2016deep, lecun2015deep, bengio2013representation, minsky1961steps, newell1956logic}. While large language models have demonstrated remarkable capabilities in reasoning and planning \citep{brown2020gpt3, openai2023gpt4, wei2022chain, chowdhery2022palm, touvron2023llama, touvron2023llama2, anil2023palm, team2023gemini, anthropic2024claude, dubey2024llama3, yang2024qwen2, grattafiori2024llama32, jiang2023mistral, team2024gemma, abdin2024phi3, achiam2023gpt4, devlin2019bert}, their ability to operate effectively in spatial contexts remains a fundamental challenge \citep{chen2024spatialvlm, yang2025embodiedbench, huang2023voxposer, huang2023visual, sharma2022vima, liu2024moka, cheng2024spatialrgpt, yang2024llmgrounder, zha2025llm3d, zhang2025open3dvqa}.

The emergence of multimodal foundation models has accelerated progress in visual understanding \citep{radford2021clip, liu2023llava, li2023blip2, alayrac2022flamingo, chen2022pali, openai2023gpt4v, bai2023qwenvl, dai2023instructblip, zhu2023minigpt4, wang2023cogvlm, liu2024llavanext, chen2024internvl2, wang2024cogvlm2, laurencon2024idefics2, ye2024mplugowl2, dosovitskiy2021vit, he2022mae, caron2021dino, dehghani2023scaling, chen2023visual, chen2024allava}, yet translating this understanding into effective spatial action remains challenging. The gap between language-based reasoning and physical world interaction represents one of the most significant obstacles to achieving truly capable autonomous systems \citep{ahn2022saycan, brohan2023rt2, driess2023palme, liang2023code, song2023llmplanner, mu2024embodiedgpt, kawaharazuka2025vla}.

We define \textbf{Agentic AI} as systems exhibiting goal-directed behavior through autonomous decision-making, characterized by four core capabilities: persistent memory for experience accumulation, planning for action sequencing, tool use for capability extension, and self-reflection for continuous improvement \citep{wang2024survey, xi2023rise, weng2023agent, yao2023react, shinn2023reflexion, park2023generative, sumers2024cognitive, wu2023autogen, hong2023metagpt, durante2024agent, guo2024large, huang2024understanding}. These agents operate through iterative cycles of perception, reasoning, action, and feedback, enabling complex task completion in dynamic environments \citep{yao2023reactsynergizingreasoningacting, shinn2023reflexionlanguageagentsverbal, madaan2023selfrefine, yao2023retroformer}.

Complementarily, \textbf{Spatial Intelligence} encompasses the ability to perceive 3D structure, reason about object relationships, navigate environments, and manipulate physical objects \citep{chen2024spatialvlm, thompson2025rem, kriegel2007spatial, ishak2008role, hegarty2006human, newcombe2010spatial, chen2024spatialreasoning, kamath2023whatsleft, liu2023visualspatial, yang2020spatialsense, marr1982vision}. This includes understanding geometric relationships, predicting physical dynamics, and planning actions that account for spatial constraints \citep{battaglia2018relational, sanchez2020learning, li2019propagation, kipf2018neural, scarselli2009graph, gilmer2017neural}.

The convergence of these domains is essential for real-world AI applications across multiple sectors. Autonomous vehicles must perceive dynamic environments and plan safe trajectories \citep{hu2023uniad, caesar2020nuscenes, sun2020waymoopen, waymo2023, tesla2023fsd, jiang2023vad, tian2024drivevlm, waymo_emma_2024, chen2024endtoend, geiger2012kitti, dosovitskiy2017carla, pomerleau1988alvinn, bojarski2016endtoend, gulino2023waymax, seff2023motionlm, mu2024most, xie2025s4driver, liu2024crash, tesla_ai_2026, cadena2016slam, campos2021orb3, mur2015orb, mur2017orb}. Robotic assistants require understanding of object affordances and spatial relationships \citep{brohan2023rt2, ahn2022saycan, brohan2022rt1, team2024octo, kim2024openvla, driess2023palme, zeng2021transporter, black2024pi0, zhen20243dvla, wu2023tidybot, huang2023instruct2act, lin2023text2motion, wake2023gpt4vision, kawaharazuka2025vla, xu2024roboticsfm, calli2015ycb, fang2020graspnet, eppner2021acronym, mahler2017dexnet, morrison2018closing}. Urban computing systems must model complex spatio-temporal dependencies \citep{jin2023stgnn, li2018dcrnn, yu2018stgcn, wu2019graphwavenet, bai2020agcrn, zheng2014urban, yuan2020survey, zheng2015trajectory, wang2020deep, atluri2018spatiotemporal, komarovsky2025spatiotemporal, balachandar2025urbanincidentpredictiongraph, cini2023taming, choi2022stgrat, cui2024spatiotemporalinteraction}. Geospatial intelligence platforms must analyze satellite imagery and geographic data at scale \citep{jakubik2024prithvi, cong2022satmae, mai2023opportunities, janowicz2020geoai, xiao2025foundationmodelsremotesensing, janowicz2025geofm, mai2024opportunities, schottlander2025geospatial, li2025autonomousgis, bai2024geogpt, bastani2023satlas, bastani2023satlaspretrain, ayush2021geography, brown2021geospatial, burke2021satellite, burke2021using}. Despite this importance, existing surveys treat these areas in isolation, lacking a unified framework connecting agentic architectures with spatial requirements.

\textbf{Contributions.} This survey makes five primary contributions:
\begin{enumerate}[leftmargin=*]
    \item A \textbf{unified taxonomy} connecting agentic AI components (memory, planning, tool use, self-reflection) with spatial intelligence domains (navigation, scene understanding, manipulation, geospatial analysis), providing a structured framework for interdisciplinary research.
    \item A \textbf{comprehensive analysis} of over 900 papers identifying key architectural patterns, including the emergence of GNN-LLM integration, vision-language-action models, and world model-based planning as critical enablers for spatial reasoning.
    \item A \textbf{systematic review} of foundation models for spatial intelligence, covering vision-language models, 3D understanding models, and geospatial foundation models.
    \item The \textbf{proposal of a unified evaluation framework, SpatialAgentBench}, with 8 tasks spanning navigation, manipulation, scene understanding, and geospatial reasoning to standardize cross-domain assessment.
    \item A \textbf{forward-looking roadmap} identifying open challenges and research directions for developing robust, safe, and capable spatially-aware autonomous systems.
\end{enumerate}

\section{Methodology}

This survey follows a systematic literature review methodology consistent with best practices in computer science \citep{kitchenham2004procedures, petersen2008systematic, wohlin2014guidelines, keele2007guidelines, brereton2007lessons, dyba2007applying, moher2009preferred}. We queried major academic databases including Google Scholar, arXiv, ACM Digital Library, IEEE Xplore, Semantic Scholar, and DBLP with keywords including ``agentic AI,'' ``spatial intelligence,'' ``embodied AI,'' ``vision-language navigation,'' ``robot manipulation,'' ``geospatial AI,'' ``world models,'' ``graph neural networks,'' ``spatio-temporal learning,'' ``vision-language-action,'' and ``foundation models for robotics.'' Our initial search yielded over 3,000 papers.

We then applied a rigorous multi-stage filtering process:

\begin{enumerate}
    \item \textbf{Temporal Filtering:} We selected papers published between 2018 and 2026, with emphasis on recent advances while including foundational works that established key paradigms.
    \item \textbf{Venue Filtering:} We prioritized papers from top-tier venues including NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, CoRL, RSS, IROS, ICRA, ACM Computing Surveys, IEEE TPAMI, Nature, Science, Science Robotics, and leading arXiv preprints.
    \item \textbf{Quality Filtering:} We prioritized papers with high citation counts, those representing foundational methods, and state-of-the-art contributions that advance the field.
    \item \textbf{Relevance Filtering:} We ensured papers directly addressed the intersection of agentic capabilities and spatial intelligence.
\end{enumerate}

This process resulted in a final corpus of over 900 papers, which were systematically analyzed to derive the taxonomy, identify key trends, and synthesize the findings presented in this survey. We employed a snowball sampling technique to ensure comprehensive coverage of related works, following citation chains both forward and backward. Two independent reviewers validated the paper selection and taxonomy development.

\section{Related Work}

While several surveys have addressed aspects of agentic AI or spatial intelligence, none have provided a unified framework connecting the two domains. We review existing surveys across five categories.

\textbf{Agentic AI Surveys.} Recent surveys on LLM-based agents \citep{wang2024survey, xi2023rise, weng2023agent, guo2024large, durante2024agent, huang2024understanding, bo2024survey, chu2024survey} focus on reasoning and tool use but do not address spatial capabilities. \citet{sumers2024cognitive} provides a cognitive architecture perspective. \citet{chan2023chateval} evaluates conversational agents.

\textbf{Embodied AI Surveys.} Embodied AI surveys \citep{duan2022surveyembodiedaisimulators, gupta2021embodied, amin2024embodied, francis2022corl, liu2024aligning, guo2024embodied} cover simulation environments and benchmarks but lack integration with agentic architectures. \citet{kawaharazuka2025vla} surveys vision-language-action models specifically for robotics.

\textbf{Geospatial AI Surveys.} Geospatial AI surveys \citep{jiang2023geospatial, li2023deep, de2021deep, yuan2021deep, mai2023opportunities, hu2019geoai, janowicz2020geoai, mai2024opportunities, janowicz2025geofm, xiao2025foundationmodelsremotesensing, schottlander2025geospatial, guan2024survey} and spatio-temporal data mining reviews \citep{jin2023stgnn, jin2023spatiotemporalgraphneuralnetworks, atluri2018spatiotemporal, wang2020deep, jiang2021graph, tedjopurnomo2020survey, ye2021spatial, xie2020urban, sahili2023spatiotemporalgraphneuralnetworks, komarovsky2025spatiotemporal} are highly specialized and do not connect to general agentic systems. \citet{zhan2024neuralnetworksgeospatialdata} surveys neural networks for geospatial data.

\textbf{Graph Neural Network Surveys.} GNN surveys \citep{wu2020gnnsurvey, zhou2020gnnsurvey, bronstein2021geometric, hamilton2020graph, zhang2020deep_gnn, liu2022graph, xia2021graph, wu2022graph, battaglia2018relational, chen2020spatial} provide comprehensive coverage of graph learning but do not focus on spatial applications or agent integration. Surveys on GNNs for specific domains include traffic \citep{jiang2022graph, rahmani2023graph, jiang2022gnn, jiang2023graph, Jiang_2022}, urban computing \citep{balachandar2025urbanincidentpredictiongraph}, and spatio-temporal prediction \citep{jin2023spatiotemporalgraphneuralnetworks, sahili2023spatiotemporalgraphneuralnetworks}.

\textbf{Vision-Language Model Surveys.} Surveys on VLMs \citep{zhang2024lmms, bordes2024introduction, yin2024survey, agrawal2023large} cover multimodal understanding but do not address spatial action or embodiment. \citet{kawaharazuka2025vla} surveys vision-language-action models specifically for robotics.

Our work is the first to bridge these gaps, providing a comprehensive, structured analysis of the convergent domain of autonomous spatial intelligence.

\section{Unified Taxonomy}

We propose a two-dimensional taxonomy (Figure~\ref{fig:taxonomy}) that maps agentic capabilities to spatial task requirements, enabling systematic analysis of existing methods and identification of research gaps.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{taxonomy.png}
    \caption{A unified taxonomy connecting Agentic AI capabilities (memory, planning, tool use, self-reflection) with Spatial Intelligence domains (navigation, scene understanding, manipulation, geospatial analysis). The intersection of these dimensions defines the design space for autonomous spatial intelligence systems.}
    \label{fig:taxonomy}
\end{figure}

\subsection{Agentic AI Components}

\subsubsection{Memory Systems}

Memory enables agents to accumulate and retrieve experiential knowledge, forming the foundation for learning and adaptation. We categorize memory systems into three types: short-term, long-term, and episodic memory.

\textbf{Short-Term Memory.} In-context learning \citep{brown2020gpt3, dong2022survey, min2022rethinking, xie2022explanation, wei2023larger, olsson2022context, akyurek2023learning, dai2023gpt, liu2023pre, wang2023label} allows models to adapt to new tasks through examples in the prompt. This mechanism enables rapid adaptation without parameter updates, leveraging the attention mechanism to condition on provided demonstrations. Working memory mechanisms \citep{graves2014neural, weston2015memory, sukhbaatar2015end, kumar2016ask, miller2016key, santoro2016meta, munkhdalai2017meta, le2020self, haria2019working} enable temporary information storage during reasoning, supporting multi-step computations that exceed single forward pass capabilities.

\textbf{Long-Term Memory.} Retrieval-augmented generation \citep{lewis2020rag, packer2023memgpt, guu2020retrieval, izacard2022few, borgeaud2022improving, khandelwal2020generalization, shi2023replug, ram2023incontext, asai2023selfrag, khattab2022demonstrate, trivedi2023interleaving, yoran2023making, jiang2023active} enables knowledge persistence beyond context limits. MemGPT \citep{packer2023memgpt} introduces hierarchical memory management for extended conversations. AMEM \citep{xu2025amemagenticmemoryllm} provides agentic memory for LLMs. MemEvolve \citep{zhang2025memevolvemetaevolutionagentmemory} enables meta-evolution of agent memory. Vector databases \citep{johnson2019billion, guo2022manu, jegou2011product, malkov2018efficient, douze2024faiss, milvus2021, pinecone2023, weaviate2023} provide efficient similarity search for memory retrieval, enabling agents to access relevant past experiences.

\textbf{Episodic Memory.} Episodic memory stores specific experiences and events, enabling agents to learn from past interactions \citep{blundell2016model, pritzel2017neural, ritter2018been, fortunato2019generalization, banino2018vector, hwang2021comet, berthouze2003epigenetic, lungarella2003developmental}. This type of memory is critical for spatial agents that must remember visited locations, encountered objects, and successful action sequences \citep{fang2019scene, savinov2018episodic, chaplot2020neural}.

\textbf{Spatial Memory.} Specialized memory for spatial information includes cognitive maps \citep{tolman1948cognitive, okeefe1978hippocampus, banino2018vector}, topological representations \citep{kuipers2000spatial, choset2001topological, karkus2021differentiableslamnetlearningparticle}, and metric maps \citep{thrun2005probabilistic, durrant2006simultaneous, cadena2016slam}. Neural approaches to spatial memory include Neural SLAM \citep{chaplot2020neural, gupta2019neuralslam, chaplot2020learning, chaplot2020object}, semantic maps \citep{huang2023vlmaps, henriques2018mapnet, jatavallabhula2023conceptfusionopensetmultimodal3d, shah2023lmnav, ha2022semantic, wu2024voronav}, and scene graphs \citep{armeni2019scene, rosinol2020kimera, hughes2022hydra, gu2024conceptgraphs, conceptgraphs2024, armeni20163d, wu2021scenegraphfusion, kim2020scene3d, li2017scene, rosinol2020scene}.

\subsubsection{Planning Systems}

Planning decomposes goals into executable action sequences, enabling complex task completion. We identify four major planning paradigms.

\textbf{Chain-of-Thought Reasoning.} Step-by-step reasoning \citep{wei2022chain, wei2023chainofthoughtpromptingelicitsreasoning, kojima2022large, wang2022self, creswell2022selection, zhou2023leasttomost, zhang2023automatic, fu2023complexitybased, li2023making, chen2023program, nye2021show, cobbe2021training, ling2017program, chung2022scaling, agrawal2023large} enables systematic problem decomposition. Self-consistency \citep{wang2022self, chen2023universal, li2023selfchecker, mitchell2022enhancing, kadavath2022language, lin2022teaching} improves reliability through multiple reasoning paths. Zero-shot chain-of-thought \citep{kojima2022large} enables reasoning without demonstrations.

\textbf{Tree-Based Search.} Tree of Thoughts \citep{yao2023tree, yao2023treethoughtsdeliberateproblem, long2023large, hulbert2023using, xie2023decomposition, sel2023algorithm, zhu2023solving} explores multiple solution branches through deliberate search. Graph of Thoughts \citep{besta2023graph, lei2023boosting, yao2024tree} enables more complex reasoning structures with arbitrary connections. RAP \citep{hao2023rap, zhao2024expel, shridhar2020alfworld} combines reasoning with acting in a planning framework. Monte Carlo Tree Search variants \citep{silver2016mastering, schrittwieser2020mastering, agostinelli2019solving, anthony2017thinking, silver2017mastering, browne2012survey, kocsis2006bandit, coulom2006efficient, berner2019dota} provide principled exploration with theoretical guarantees.

\textbf{Hierarchical Planning.} LLM-Planner \citep{song2023llmplanner} enables few-shot grounded planning for embodied agents. Inner Monologue \citep{huang2022inner} provides feedback-driven planning through internal dialogue. HiPlan \citep{li2025hiplanhierarchicalplanningllmbased} introduces hierarchical planning with LLMs. Hierarchical RL approaches \citep{nachum2018data, vezhnevets2017feudal, bacon2017option, kulkarni2016hierarchical, levy2019learning, zhang2020generating, li2020skill, gupta2019relay, pertsch2021accelerating, grannen2023stabilize} decompose tasks into subtasks with temporal abstraction.

\textbf{Task and Motion Planning.} TAMP \citep{garrett2021integrated, kaelbling2020hierarchical, tennison2024grounded, dantam2016incremental, kaelbling2011hierarchical, lozano2014constraint, li2020hybrid, toussaint2015logic, srivastava2014combined, hadfield2017sequential, driess2020deep, silver2021planning, chitnis2016guided} integrates symbolic planning with continuous motion planning for robotic applications. This approach combines the expressiveness of symbolic reasoning with the precision of geometric planning.

\textbf{LLM-Based Planning.} Recent work leverages LLMs directly for planning \citep{huang2022zeroshot, huang2022language, valmeekam2023large, song2023llmplanner, liu2023llm+, silver2024generalized, huang2024understandingplanningllmagents, mao2023gpt}. SayCan \citep{ahn2022saycan} grounds language models in affordances. Code as Policies \citep{liang2023code} generates executable robot code. ProgPrompt \citep{singh2023progprompt} uses programmatic prompting for task planning.

\subsubsection{Tool Use and Action}

Tool use extends agent capabilities through external interfaces and physical actions.

\textbf{API Integration.} Toolformer \citep{schick2023toolformer} enables self-supervised tool learning. Gorilla \citep{patil2023gorilla} specializes in API calling with retrieval augmentation. ToolLLM \citep{qin2023toolllm, qin2024toolllm, qian2024graphtool} provides comprehensive tool use benchmarks. TaskMatrix \citep{liang2023taskmatrix} connects foundation models with millions of APIs. TALM \citep{parisi2022talm} augments language models with tool use. Additional tool-use frameworks include HuggingGPT \citep{shen2023hugginggpt, shen2024hugginggpt}, ToolkenGPT \citep{hao2024toolkengpt}, API-Bank \citep{li2023apibank}, Chameleon \citep{lu2023chameleon}, ViperGPT \citep{suris2023vipergpt}, Visual ChatGPT \citep{wu2023visual}, and MM-ReAct \citep{yang2023mmreact}.

\textbf{Code Generation.} PAL \citep{gao2023pal} uses code for reasoning. Code as Policies \citep{liang2023code} generates executable robot code from language. Codex \citep{chen2021evaluating, chen2021decision}, CodeGen \citep{nijkamp2023codegen}, StarCoder \citep{li2023starcoder}, CodeLlama \citep{roziere2023codellama}, WizardCoder \citep{luo2023wizardcoder}, and DeepSeek-Coder \citep{guo2024deepseek} provide code generation capabilities. ProgPrompt \citep{singh2023progprompt} uses programmatic prompting for robotics. Self-debugging \citep{chen2023teaching}, self-repair \citep{olausson2023selfrepair}, and self-play \citep{haluptzok2023language} improve code quality through iterative refinement.

\textbf{ReAct Architecture.} ReAct \citep{yao2023react, yao2023reactsynergizingreasoningacting} interleaves reasoning with action execution, enabling agents to think before acting. Reflexion \citep{shinn2023reflexion, shinn2023reflexionlanguageagentsverbal} adds self-reflection for improvement through verbal reinforcement. Additional architectures include LATS \citep{zhou2023lats}, SwiftSage \citep{lin2024swiftsage}, FireAct \citep{chen2023fireact}, and SWE-Agent \citep{yang2024sweagent}. These architectures form the foundation for many spatial agents.

\textbf{Physical Action.} For embodied agents, tool use extends to physical manipulation \citep{zeng2021transporter, shridhar2022cliport, brohan2022rt1, brohan2023rt2}. Action primitives \citep{dalal2021accelerating, nasiriany2022augmenting} provide reusable building blocks. Skill libraries \citep{wang2023voyager, pertsch2021accelerating, lynch2020learning} enable compositional action.

\subsubsection{Self-Reflection and Learning}

Self-reflection enables agents to evaluate and improve their own performance.

\textbf{Self-Critique.} Reflexion \citep{shinn2023reflexion} uses verbal self-reflection for improvement. Self-Refine \citep{madaan2023selfrefine} iteratively improves outputs through self-feedback. Constitutional AI \citep{bai2022constitutional} uses self-critique for alignment. Self-debugging \citep{chen2023teaching} enables code correction through self-analysis.

\textbf{Learning from Experience.} Voyager \citep{wang2023voyager} builds skill libraries through exploration. AutoGPT \citep{significant2023autogpt} demonstrates autonomous goal pursuit. LangChain \citep{chase2022langchain} provides frameworks for agent development. Retroformer \citep{yao2023retroformer} enables retrospective learning from failures.

\subsection{Spatial Intelligence Domains}

\subsubsection{Navigation}

Navigation requires path planning and execution in physical or simulated environments. We categorize navigation methods by input modality and task specification.

\textbf{Vision-Language Navigation.} R2R \citep{anderson2018vln, anderson2018on, anderson2019chasing} introduced the VLN task with natural language instructions in photorealistic environments. RxR \citep{ku2020room, ku2020roomacrossroommultilingualvisionandlanguagenavigation} extends to multilingual settings with diverse annotators. REVERIE \citep{qi2020reverie} adds remote object grounding requiring fine-grained understanding. TouchDown \citep{chen2019touchdown, hermann2019learningfollowdirectionsstreet, blukis2018mapping, blukis2019mapping} addresses urban street-level navigation. Speaker-Follower \citep{fried2018speaker} uses data augmentation through instruction generation. EnvDrop \citep{tan2019learning, tan-etal-2019-learning} improves generalization through environment dropout. PREVALENT \citep{hao2020prevalent} pretrains on VLN data with auxiliary tasks. VLN-BERT \citep{hong2020vlnbert, hong2021vlnbert} applies transformers to VLN. HAMT \citep{chen2021hamt} uses hierarchical attention for multi-scale reasoning. DUET \citep{chen2022duet} employs dual-scale transformers. Additional methods include RecBERT \citep{hong2020recbert}, AirBERT \citep{guhur2021airbert}, VLN-CE \citep{krantz2020navgraph, krantz2020navgraphvisionandlanguagenavigationcontinuous, krantz2020vlnce, krantz2020waypoint}, CWP \citep{hong2020sub}, BEVBert \citep{an2023bevbert}, NavGPT \citep{zhou2023navgpt, zhou2024navgpt, zhou2024navgpt2}, MapGPT \citep{chen2024mapgpt, zhang2024mapgpt}, LM-Nav \citep{shah2023lmnav}, and cross-lingual approaches \citep{yan2020crosslingualvisionlanguagenavigation}.

\textbf{Object-Goal Navigation.} Object-goal navigation requires finding specific object categories without explicit instructions \citep{batra2020objectnav, chaplot2020objectgoal, ramakrishnan2022poni, ye2021auxiliary, khandelwal2022simple, majumdar2022zson, gadre2023cow, yokoyama2024vlfm}. ZSON \citep{majumdar2022zson} enables zero-shot object navigation using CLIP embeddings. CLIP-Nav \citep{dorbala2022clipnav, dorbala2024can} leverages vision-language models for semantic navigation. CoW \citep{gadre2023cow, gadre2024cows2} uses CLIP on Wheels for open-vocabulary navigation. Semantic exploration \citep{chaplot2020semantic, ramakrishnan2022poni} builds semantic maps for efficient search. SemExp \citep{chaplot2020semantic} combines semantic mapping with goal-oriented exploration.

\textbf{Audio-Visual Navigation.} Audio-visual navigation incorporates sound for localization \citep{chen2020soundspaces, gan2020look, chen2021semantic, chen2020learning, gao2020visualechoes, huang2024audio}. SoundSpaces \citep{chen2020soundspaces, chen2022soundspaces2} provides audio simulation for embodied AI. Audio-visual embodied navigation \citep{chen2020learning, gan2020look} enables following sound sources.

\textbf{Embodied Question Answering.} EQA \citep{Das_2018_CVPR, das2018embodied, das2017embodiedquestionanswering, das2018eqa, gordon2018iqa, wijmans2019embodied, kadian2020embodiedqa} requires navigating to answer questions about environments. 3D-QA benchmarks \citep{azuma2022scanqa, ye20213d, ma2022sqa3d, fu20213dqa} extend to 3D scene understanding.

\subsubsection{Scene Understanding}

Scene understanding encompasses 3D perception, semantic segmentation, and spatial relationship reasoning.

\textbf{Neural Radiance Fields.} NeRF \citep{mildenhall2020nerf, mildenhall2020nerfrepresentingscenesneural, chen2022tensorf, barron2023zipnerf, courant2023blunfblueprintneuralfield, aoki2023improvingnerfheightdata} revolutionized novel view synthesis through implicit neural representations. Extensions include Mip-NeRF \citep{barron2021mipnerf, barron2022mipnerf360, martinbrualla2021nerfw, martin2021nerfw}, Instant-NGP \citep{muller2022instant, tancik2022blocknerf, tancik2023nerfstudio}, and Plenoxels \citep{fridovich2022plenoxels, park2021nerfies, pumarola2021dnerf}. 3D Gaussian Splatting \citep{kerbl20233dgaussian, kerbl20233dgaussiansplattingrealtime, fan2024lightgaussian, zhou2024feature3dgs, yang2024deformable3dgs} provides real-time rendering with explicit representations. SLAM integration \citep{rosinol2022nerf, rosinol20203d, rosinol2020scene, rosinol2022nerfslamrealtimedensemonocular, zhu2022nice, sucar2021imap, engel2014lsd, engel2018dso, bird2025dvmslam, johari2023eslam, keetha2024splatam, yan2024gsslam, wang2023coslam, schops2019badslam, zhang2020neuralslamlearningexplore} enables online reconstruction.

\textbf{Point Cloud Processing.} PointNet \citep{qi2017pointnet, qi2017pointnet2, qi2017pointnetpp, qi2017pointnetplusplus} and PointNet++ \citep{qi2017pointnetplusplus} established deep learning on point clouds. Point Transformer \citep{zhao2021point, zhao2021pointtransformer, wu2022point, wu2022pointtransformerv2, wu2024pointtransformerv3} applies attention mechanisms. Point-BERT \citep{yu2022pointbert, shen2023point}, Point-MAE \citep{pang2022masked, pang20223d, zhang2022pointm2ae}, and PointGPT \citep{chen2024pointgpt, qian2022pointnext} enable self-supervised pretraining. 3D semantic segmentation \citep{Graham_2018_CVPR, choy2019minkowski, thomas2019kpconv, hu2020randla, li2018pointcnn, guo2021pct} processes large-scale point clouds.

\textbf{Depth Estimation.} Monocular depth estimation \citep{Godard_2019_ICCV, godard2017unsupervised, godard2019digging, ranftl2021dpt, ranftl2021vision, bhat2021adabins, yang2024depth, eigen2014depth} enables 3D understanding from single images. MiDaS \citep{ranftl2020midas} provides robust cross-dataset depth estimation. Depth Anything \citep{yang2024depth} scales depth estimation with large datasets.

\textbf{Semantic Segmentation.} Semantic segmentation \citep{long2015fcn, chen2017deeplab, chen2018encoder, xie2021segformer, cheng2022masked, ke2024segment3d} provides pixel-level scene understanding. Open-vocabulary segmentation \citep{ghiasi2022scaling, xu2022simple, liang2023open, xu2020scene, xu2017scene, xu2017scenegraph} enables recognition of novel categories.

\subsubsection{Manipulation}

Manipulation involves physical interaction with objects through robotic systems.

\textbf{Vision-Language-Action Models.} RT-1 \citep{brohan2022rt1, argall2009survey, billard2008survey, billard2019trends} demonstrated large-scale robot learning with transformer architectures. RT-2 \citep{brohan2023rt2} showed VLM transfer to robotic control. RT-X \citep{open_x_embodiment_rt_x_2023, padalkar2023openx} enables cross-embodiment transfer across 22 robot platforms. Octo \citep{team2024octo} provides an open-source generalist robot policy. OpenVLA \citep{kim2024openvla} offers open vision-language-action models. $\pi_0$ \citep{black2024pi0} demonstrates flow matching for robot control. RoboCat \citep{bousmalis2023robocat} shows self-improvement through autonomous practice. Additional VLA models include 3D-VLA \citep{zhen20243dvla}, GR-1 \citep{wu2023gr1}, RoboFlamingo \citep{li2023vision}, ManipLLM \citep{li2024manipllm}, GraspGPT \citep{wu2023graspgpt, wu2024graspgpt}, and comprehensive surveys \citep{sapkota2025visionlanguageactionmodelsconceptsprogress, ma2026surveyvisionlanguageactionmodelsembodied}.

\textbf{Imitation Learning.} Behavior cloning \citep{pomerleau1988alvinn, bojarski2016endtoend, florence2022implicit, argall2009survey, hussein2017imitation} learns policies from demonstrations. Diffusion Policy \citep{chi2023diffusion, chi2024diffusion} applies diffusion models to action generation. ACT \citep{zhao2023aloha} enables learning from teleoperation. Learning from play \citep{lynch2020learning, lynch2023interactive} leverages unstructured demonstrations. ALOHA \citep{zhao2023aloha} provides bimanual teleoperation for data collection.

\textbf{Reinforcement Learning.} Model-free RL \citep{schulman2017ppo, haarnoja2018sac, fujimoto2018td3, kalashnikov2018qtopt, rafailov2023dpo} provides policy optimization. Model-based RL \citep{hafner2019dreamer, hafner2023dreamerv3, janner2019mbpo} enables sample-efficient learning. Offline RL \citep{levine2020offline, kumar2020conservative, kostrikov2021offline} learns from fixed datasets. Sim-to-real transfer \citep{tobin2017domain, james2019sim, andrychowicz2020learning, akkaya2019rubiks} bridges simulation and reality.

\textbf{Grasping and Manipulation.} Grasp detection \citep{mahler2017dexnet, fang2020graspnet, sundermeyer2021contact, morrison2018closing} enables object pickup. Dexterous manipulation \citep{akkaya2019rubiks, chen2022system, handa2020dexpilot, chen2022dexcap, dahiya2010tactile, qin2023dexpoint} handles complex object interactions. Language-conditioned manipulation \citep{shridhar2022cliport, lynch2023interactive, jang2022bc, jang2022bcz, rana2023sayplan, wake2023chatgpt, wake2023gpt4v, vemprala2024chatgpt, xiao2023robot} follows natural language instructions.

\textbf{Simulation Environments.} RLBench \citep{james2020rlbench, batra2020rearrangementchallengeembodiedai}, Meta-World \citep{yu2020metaworld}, ManiSkill \citep{mu2021maniskill, gu2023maniskill2}, Isaac Gym \citep{makoviychuk2021isaac}, RoboTHOR \citep{deitke2020robothor, deitke2022procthor}, and AI2-THOR \citep{kolve2017ai2thor} provide simulation platforms. Objaverse \citep{deitke2023objaverse, deitke2024objaversexl} and ShapeNet \citep{chang2015shapenet} provide 3D object assets.

\textbf{Emerging Directions.} Recent work explores scaling laws for robotics \citep{brohan2023rt2, baniodeh2025scaling}, cross-embodiment transfer \citep{open_x_embodiment_rt_x_2023, padalkar2023openx}, integration with world models \citep{wu2023daydreamer}, and foundation models for robotics \citep{xu2024roboticsfm, kawaharazuka2025vla, bharadhwaj2024roboagent, sciortino2023vision}. Scene-level understanding \citep{tan2025scenediffuserplusplus} and motor control integration \citep{liu2022motor} represent additional frontiers.

\subsubsection{Geospatial Analysis}

Geospatial analysis processes geographic data including satellite imagery, maps, and location data.

\textbf{Remote Sensing Foundation Models.} Prithvi \citep{jakubik2024prithvi, jakubik2023prithvi, guo2024skysense, hong2024spectralgpt} provides geospatial foundation models for Earth observation. SatMAE \citep{cong2022satmae, fuller2022satclip} applies masked autoencoders to satellite imagery. GeoFM \citep{janowicz2025geofm, zhan2024neuralnetworksgeospatialdata} surveys foundation models for geospatial applications. CROMA \citep{fuller2024croma, cheng2017rsicd} enables cross-modal remote sensing analysis. GeoChat \citep{kuckreja2024geochat}, RemoteSenseGPT \citep{liu2024remotesensegpt}, SkyEyeGPT \citep{zhan2024skyeyegpt}, and EarthGPT \citep{zhang2024earthgpt} apply LLMs to remote sensing. GeoLLM \citep{manvi2024geollm, yin2024geollm} enables geospatial reasoning with language models. RemoteCLIP \citep{mall2023remoteclip} and GeoCLIP \citep{vivanco2024geoclip} provide vision-language models for remote sensing. Microestimates \citep{chi2022microestimates, burke2021using} derive socioeconomic indicators from satellite data.

\textbf{Spatio-Temporal Graph Neural Networks.} DCRNN \citep{li2018dcrnn} models traffic as bidirectional graph diffusion. STGCN \citep{yu2018stgcn} combines graph and temporal convolutions. Graph WaveNet \citep{wu2019graphwavenet} learns adaptive graph structures. AGCRN \citep{bai2020agcrn} introduces node-specific patterns. STGRAT \citep{park2020stgrat} applies attention for spatio-temporal prediction. Comprehensive surveys \citep{jin2023stgnn, jin2023spatiotemporalgraphneuralnetworks, sahili2023spatiotemporalgraphneuralnetworks, choi2022graph, ni2023deep} detail these advances.

\textbf{Urban Computing.} Urban computing \citep{zheng2014urban, zheng2011urban, yuan2020survey, bibri2017smart, li2022urban, liang2019urbanfm, pan2019urbanfm} applies AI to city-scale problems. Traffic prediction \citep{li2018dcrnn, yu2018stgcn, wu2019graphwavenet, li2023trafficbert}, POI recommendation \citep{lian2020geography, zhao2020go, feng2020scalable}, and urban flow prediction \citep{zhang2017deep, yao2018deep, cheng2018crowd} are key applications. ST-LLM \citep{liu2024stllm, yuan2024stllm} applies large language models to spatio-temporal data. UniST \citep{yuan2024unist, chen2022z-gcnets} provides unified spatio-temporal learning. UrbanGPT \citep{li2024urbangpt} enables urban prediction with LLMs. GeographRAG \citep{liang2025geographrag} combines retrieval with geospatial reasoning. Spatio-temporal interaction \citep{wu2024spatial, wu2021spatial} models complex urban dynamics.

\section{State-of-the-Art Methods}

\subsection{Graph Neural Networks for Spatial Reasoning}

GNNs provide powerful tools for modeling spatial relationships and dependencies across diverse domains. We present the mathematical foundations underlying these architectures.

\textbf{Message Passing Framework.} The general message passing neural network (MPNN) framework \citep{gilmer2017neural, gilmer2017mpnn, battaglia2018relational} defines node updates through:
\begin{equation}
\mathbf{h}_v^{(l+1)} = \phi\left(\mathbf{h}_v^{(l)}, \bigoplus_{u \in \mathcal{N}(v)} \psi\left(\mathbf{h}_v^{(l)}, \mathbf{h}_u^{(l)}, \mathbf{e}_{uv}\right)\right)
\end{equation}
where $\mathbf{h}_v^{(l)}$ is the hidden state of node $v$ at layer $l$, $\mathcal{N}(v)$ denotes the neighbors of $v$, $\psi$ is the message function, $\bigoplus$ is a permutation-invariant aggregation (e.g., sum, mean, max), and $\phi$ is the update function.

\textbf{Foundational Architectures.} GCN \citep{kipf2017gcn} introduced spectral graph convolution through first-order Chebyshev polynomial approximation:
\begin{equation}
\mathbf{H}^{(l+1)} = \sigma\left(\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)}\right)
\end{equation}
where $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$ is the adjacency matrix with self-loops, $\tilde{\mathbf{D}}$ is the degree matrix, and $\mathbf{W}^{(l)}$ are learnable parameters.

GAT \citep{velickovic2018gat} added attention mechanisms for adaptive aggregation:
\begin{equation}
\alpha_{ij} = \frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j]\right)\right)}{\sum_{k \in \mathcal{N}(i)} \exp\left(\text{LeakyReLU}\left(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_k]\right)\right)}
\end{equation}

GraphSAGE \citep{hamilton2017graphsage} enabled inductive learning on unseen nodes through neighborhood sampling. GIN \citep{xu2019gin} provided theoretical expressiveness analysis connecting to Weisfeiler-Lehman tests. Additional architectures include SGC \citep{wu2019simplifying, emmons2020sparse}, APPNP \citep{klicpera2019appnp}, GPR-GNN \citep{chien2021gprgnn}, spectral approaches \citep{cao2020spectral, Chen_2019}, graph attention \citep{fan2019graph, Kang2020GraphRN}, and comprehensive surveys \citep{wu2020gnnsurvey, zhou2020gnnsurvey, battaglia2018relational, zhang2019graph, yang2018graph, wu2019graph, zhou2020graph}. Graph reasoning \citep{gowda2025graphs, Yang2025D3GNN, ma2023graph, lee2023graph} extends capabilities.

\textbf{Spatio-Temporal Networks.} DCRNN \citep{li2018dcrnn} models traffic as bidirectional graph diffusion:
\begin{equation}
\mathbf{H}^{(l)} = \sum_{k=0}^{K} \left(\mathbf{P}_f^k \mathbf{X} \mathbf{W}_{k,1} + \mathbf{P}_b^k \mathbf{X} \mathbf{W}_{k,2}\right)
\end{equation}
where $\mathbf{P}_f$ and $\mathbf{P}_b$ are forward and backward transition matrices.

STGCN \citep{yu2018stgcn} combines graph and temporal convolutions through a sandwiched structure. Graph WaveNet \citep{wu2019graphwavenet} learns adaptive graph structures without predefined adjacency:
\begin{equation}
\tilde{\mathbf{A}} = \text{SoftMax}\left(\text{ReLU}\left(\mathbf{E}_1 \mathbf{E}_2^T\right)\right)
\end{equation}
where $\mathbf{E}_1, \mathbf{E}_2$ are learnable node embeddings.

AGCRN \citep{bai2020agcrn} introduces node-specific patterns through adaptive modules. ASTGCN \citep{guo2019astgcn, guo2019attention} adds spatial and temporal attention mechanisms. GMAN \citep{zheng2020gman} uses graph multi-attention with transform attention for long-range dependencies. Comprehensive surveys \citep{jin2023stgnn, jin2023spatiotemporalgraphneuralnetworks, atluri2018spatiotemporal, wang2020deep, jiang2021graph, sahili2023spatiotemporalgraphneuralnetworks, komarovsky2025spatiotemporal, wang2020traffic, li2021spatiotemporalgraphdualattentionnetwork, singh2025leveragingspatiotemporalgraphneural} detail these advances.

\textbf{GNN-LLM Integration.} Emerging work combines GNNs with LLMs for structured spatial reasoning \citep{chen2024llaga, tang2024graphgpt, ye2024language, fatemi2023talk, fatemi2024talk, huang2024can, perozzi2024let, perozzi2024letgraph, chen2019knowledge, huang2024gnngpt, jin2024llmkgsurvey}. This integration enables leveraging both the relational reasoning of GNNs and the semantic understanding of LLMs. Graph instruction tuning \citep{zhang2024graphinstruct, zhao2023graphtext, zhao2024graphtext, chen2023exploring, mao2024graphinstruct, li2024graphwiz, li2024graphreader, liu2024gpt4graph, zhang2024llmgraph, ren2024graphsurvey} further enhances this capability. LLaGA \citep{chen2024llaga} provides language-graph alignment. GraphGPT \citep{tang2024graphgpt, tang2024graphllm, chen2024graphgpt, chai2023graphllm, tang2024higpt, jiang2024structgpt} enables graph reasoning through language models. Knowledge graph integration \citep{guo2024gpt4kg, bosselut2019comet, bordes2013transe, chen2020review, pan2024unifyingllmkg, sun2024thinkongraph, wei2024llmgraph} extends semantic reasoning.

\textbf{Geometric Deep Learning.} Geometric deep learning \citep{bronstein2021geometric, han2024geometric} provides theoretical foundations for spatial reasoning on non-Euclidean domains. Equivariant networks \citep{cohen2016group, weiler2019general, fuchs2020se3} respect spatial symmetries through:
\begin{equation}
f(T_g \cdot x) = T_g \cdot f(x)
\end{equation}
where $T_g$ is a group transformation. Graph transformers \citep{shehzad2024graphtransformers, ying2021transformers, ying2021graphormer, dwivedi2023benchmarking, Zorzi_2022_CVPR, mialon2021graphit, zhang2024graphfoundation} combine attention with graph structure. E3NN \citep{batzner2022e3nn} and geometric message passing \citep{brandstetter2022geometric} advance equivariant architectures.

\subsection{World Models}

World models learn predictive representations enabling planning through imagination.

\textbf{Latent Dynamics Models.} World models learn a latent dynamics model that predicts future states:
\begin{align}
\text{Encoder:} \quad & \mathbf{z}_t = q_\phi(\mathbf{z}_t | \mathbf{o}_{\leq t}, \mathbf{a}_{<t}) \\
\text{Dynamics:} \quad & \hat{\mathbf{z}}_{t+1} = p_\theta(\hat{\mathbf{z}}_{t+1} | \mathbf{z}_t, \mathbf{a}_t) \\
\text{Decoder:} \quad & \hat{\mathbf{o}}_t = p_\psi(\hat{\mathbf{o}}_t | \mathbf{z}_t)
\end{align}

\textbf{Model-Based Reinforcement Learning.} Dreamer \citep{hafner2019dreamer, hafner2019learning, hafner2020dreamerv1, hafner2019dream} introduced latent imagination for sample-efficient learning through recurrent state-space models. DreamerV2 \citep{hafner2021dreamerv2} achieved human-level Atari performance with discrete latent states. DreamerV3 \citep{hafner2023dreamerv3} demonstrated cross-domain mastery with a single algorithm through symlog predictions. DayDreamer \citep{wu2023daydreamer} transferred world models to real robots with minimal real-world data. PlaNet \citep{hafner2019learning} pioneered latent dynamics learning. MuZero \citep{schrittwieser2020mastering} combined learned models with MCTS for game playing. Additional approaches include MBPO \citep{janner2019mbpo}, SLAC \citep{lee2020slac}, TD-MPC \citep{hansen2022tdmpc}, and World Models \citep{ha2018worldmodels, ha2018world, gregor2019shaping}.

\textbf{Video World Models.} Genie \citep{bruce2024genie} learns controllable world models from internet videos enabling interactive environments. WorldDreamer \citep{yang2024worlddreamer} generates driving world models for autonomous vehicles. GAIA-1 \citep{hu2023gaia1} produces realistic driving videos conditioned on actions and text. Sora \citep{openai2024sora, brooks2024sora} demonstrates video generation as world simulation at scale. Video prediction models \citep{yang2024video, yang2024worldmodels, baker2022video, xiang2024pandora, du2024learning, min2024driveworld, wang2024worldsim} provide foundations for world understanding.

\textbf{LLM-Based World Models.} LLMs can serve as world models for planning \citep{hao2023rap, guan2023leveraging, huang2022language, yang2023learningworld}, predicting state transitions without explicit environment models. This approach leverages the vast knowledge encoded in LLMs to simulate world dynamics. RAP \citep{hao2023rap} combines reasoning with acting through world model rollouts. TransDreamer \citep{chen2022transdreamer} and UniSim \citep{yang2023unisim} advance world modeling.

\textbf{World Models for Robotics.} World models for robotics \citep{feng2025worldmodels, ding2024worldmodels, li2025worldmodels, barnes2023world, gregor2019shaping, eslami2018neural, du2023improving, liu2024worldmodel, lorica2025world, reply2026world, zhen2024worldmodel3d, li2024worldcentric} enable sample-efficient learning and safe exploration. World models for autonomous driving \citep{wang2024worldmodelsad, ngiam2022scene} provide simulation for planning.

\subsection{Multimodal Foundation Models}

Multimodal models integrate vision, language, and action understanding.

\textbf{Vision-Language Models.} CLIP \citep{radford2021clip, gadre2022clip} enabled zero-shot visual recognition through contrastive pretraining on web-scale data. BLIP-2 \citep{li2023blip2} introduced efficient vision-language pretraining with frozen encoders. LLaVA \citep{liu2023llava, liu2024llavanext} demonstrated visual instruction tuning with strong performance. GPT-4V \citep{openai2023gpt4v, zheng2024gpt4vision, yan2023gpt4v} achieved strong multimodal reasoning. Gemini \citep{team2023gemini, google2024gemini, efron2025google} provides native multimodal capabilities. Flamingo \citep{alayrac2022flamingo} enables few-shot visual learning through interleaved attention. PaLI \citep{chen2022pali, chen2023pali, chen2024pali3, chen2024scalable} scales vision-language models. Kosmos-2 \citep{peng2023kosmos2, microsoft2023kosmos} adds grounding capabilities. Qwen-VL \citep{bai2023qwenvl} provides open multilingual VLMs. Additional models include InstructBLIP \citep{dai2023instructblip}, MiniGPT-4 \citep{zhu2023minigpt4, zhu2024minigpt4, chen2024minigptv2, li2024minigpt5}, Otter \citep{li2023otter}, CogVLM \citep{wang2023cogvlm, wang2024cogvlm2, hong2024cogagent}, InternVL \citep{chen2023internvl, chen2024internvl2, chen2024internvl}, IDEFICS2 \citep{laurencon2024idefics2}, mPLUG-Owl2 \citep{ye2024mplugowl2, ye2024mplugowl}, Ferret \citep{you2024ferret}, and VisionLLM \citep{wang2024visionllm, wang2024visionllmv2}.

\textbf{Spatial Vision-Language Models.} SpatialVLM \citep{chen2024spatialvlm, chen2024spatialvlm2} specializes in spatial reasoning with fine-grained understanding. SpatialRGPT \citep{cheng2024spatialrgpt} provides regional spatial reasoning. VoxPoser \citep{huang2023voxposer} extracts affordances from VLMs into 3D representations. VLMaps \citep{huang2023vlmaps} creates semantic spatial maps for navigation. These models bridge vision-language understanding with spatial reasoning.

\textbf{3D Vision-Language Models.} 3D-LLM \citep{hong20233dllm, chen2024ll3da, fu2024scenellm, wang2023chat3d, zhang2024llava3d, zheng2024llava3d} enables language understanding of 3D scenes. Open3D-VQA \citep{zhang2025open3dvqa, peng2023openscene, peng2023openscene2, takmaz2023openmask3d} provides open-vocabulary 3D visual question answering. LLM-Grounder \citep{hong2024llmgrounder, yang2024llmgrounder, huang2024leo} grounds language in 3D environments.

\textbf{Evaluation Benchmarks.} VLM evaluation benchmarks include MMMU \citep{yue2024mmmu, Krishna_2017_ICCV_visual_genome, chan2023chateval}, MathVista \citep{lu2024mathvista}, MME \citep{fu2024mme}, SEED-Bench \citep{li2024seedbench, li2024seedbench2}, MMBench \citep{liu2024mmbench}, and LMMS-Eval \citep{zhang2024lmms}.

\subsection{Embodied AI Agents}

\textbf{Open-Ended Exploration.} Voyager \citep{wang2023voyager, chen2024far, stone2023openworld} demonstrated open-ended exploration in Minecraft through LLM-driven curriculum learning and skill library construction. MineDojo \citep{fan2022minedojo, gan2021threedworld} provides benchmarks for open-ended embodied agents with diverse tasks. DEPS \citep{wang2023deps} decomposes embodied planning systematically. MineAnyBuild \citep{mineanybuild2025} extends to construction tasks.

\textbf{Grounded Language Agents.} SayCan \citep{ahn2022saycan} grounds language models in robotic affordances through value functions. Code as Policies \citep{liang2023code} generates executable robot code from language. LLM-Planner \citep{song2023llmplanner} enables few-shot grounded planning. EmbodiedGPT \citep{mu2024embodiedgpt, mu2023embodiedgpt, huang2023embodied} provides embodied chain-of-thought reasoning. RoboBrain \citep{ji2025robobrain, li2024embodiedworld} integrates multiple capabilities.

\textbf{Multi-Agent Systems.} AutoGen \citep{wu2023autogen, talebirad2023multiagent, li2024survey_multiagent} enables multi-agent conversations with flexible architectures. MetaGPT \citep{hong2023metagpt, hong2024metagpt} assigns roles to agents for software development. CAMEL \citep{li2023camel, li2023s} explores communicative agents through role-playing. ChatDev \citep{qian2023communicative, qian2024chatdev, wang2024llmcoordination} applies multi-agent systems to software development. AgentVerse \citep{chen2024agentverse, chen2023agentverse, chitnis2020efficient} provides multi-agent simulation frameworks. Multi-agent collaboration \citep{dang2025multiagentcollaborationevolvingorchestration, chen2024maagent, A_multi_agent_system_for_enterprise_integration, Borges2014MultiagentGS, hallyburton2025trusteddatafusionmultiagent} extends coordination capabilities.

\textbf{Web Agents.} WebArena \citep{zhou2023webarena, he2024webvoyager, rawles2024androidworld} benchmarks web-based agent tasks. Mind2Web \citep{deng2024mind2web, liu2024visualagentbench} provides web agent datasets. SWE-Bench \citep{jimenez2024swebench, yang2024swebenchverified} evaluates software engineering agents. SWE-Agent \citep{yang2024sweagent} provides agent interfaces for software tasks. WebGPT \citep{nakano2021webgpt} enables web browsing for question answering. OSWorld \citep{xie2024osworld, wang2024osworld, fu2024mobile}, WorkArena \citep{drouin2024workarena}, RealWorld \citep{gur2024realworld}, and SeeClick \citep{cheng2024seeclick, yang2023autogptonlinedecisionmaking} extend web agent benchmarks.

\section{Industry Applications}

\subsection{Geospatial Intelligence}

\textbf{Palantir} \citep{palantir2023, bailey2021palantir, palantir_foundry_2023, palantir2024gotham, freeman2021palantir, chen2020application} integrates AI with geospatial analysis for defense and commercial applications, processing satellite imagery and sensor data at scale. The Gotham platform enables intelligence analysis with spatial reasoning. \textbf{ESRI} \citep{esri2023, esri_arcgis_2023, esri2024geoai, esri_geospatial_ai_2024, li2020geoai} provides ArcGIS with integrated GeoAI capabilities for spatial analysis, supporting urban planning, environmental monitoring, and disaster response. \textbf{Google} \citep{googlemaps2023, google_earth_2023, google2024mapsai, google_gemini_maps_2025, google_geospatial_ai_2025, downs2022google, robotreport2025gemini} deploys AI for global-scale mapping, navigation, and earth observation through Google Earth Engine and Maps AI. Google DeepMind advances embodied AI through robotics research \citep{brohan2022rt1, brohan2023rt2, open_x_embodiment_rt_x_2023}.

\textbf{Satellite and Earth Observation.} \textbf{Maxar} provides high-resolution satellite imagery for geospatial intelligence and defense applications. \textbf{Planet Labs} \citep{planet_labs_2023} operates the largest constellation of Earth-imaging satellites, enabling daily global monitoring for agriculture, forestry, and disaster response. \textbf{NASA} \citep{nasa_earthdata_2023, jakubik2024prithvi} develops foundation models for Earth science through partnerships with IBM, including the Prithvi geospatial foundation model trained on Harmonized Landsat Sentinel-2 data. \textbf{IBM} collaborates on geospatial AI through the NASA-IBM partnership \citep{jakubik2024prithvi, jakubik2023prithvi}, developing open-source foundation models for climate and Earth observation.

\textbf{Defense Applications.} DARPA programs including MCS \citep{darpa_mcs}, ARM \citep{darpa_arm}, and GCA \citep{darpa_gca} advance spatial AI for defense. NATO applications \citep{weingarten_nato} integrate geospatial intelligence. GEOINT applications \citep{lutema_geoint} leverage satellite imagery analysis.

\textbf{Disaster Response.} UN-Habitat \citep{UNHabitat2025} applies AI for urban resilience. UNDRR \citep{undrr_ai_tools_2025} provides AI tools for disaster risk reduction. FEMA \citep{fema_geospatial_damage_2025} uses geospatial AI for damage assessment. Urban SDK \citep{urbansdk_ai_disaster_planning} enables AI-powered disaster planning. Agentic AI for SAR \citep{elshenety_agentic_ai_sar} supports search and rescue operations.

\subsection{Location Intelligence}

\textbf{Foursquare} \citep{foursquare2023, foursquare_unfolded_2023, foursquare2024places} provides location intelligence through movement pattern analysis and POI data, powering applications across retail, real estate, and urban analytics. \textbf{Wherobots} \citep{wherobots_2024} provides cloud-native spatial analytics built on Apache Sedona, enabling large-scale geospatial data processing. \textbf{Carto} \citep{carto_2024} offers spatial data science platforms for location intelligence and geospatial analysis. Smart city applications \citep{zheng2014urban, allam2020ai, shafique2020internet, zanella2014internet, bibri2017smart, huang2021urban, li2018deepurban, pan2019urban, silva2018urban} leverage spatial AI for traffic management, energy optimization, and urban planning.

\textbf{Mapping and Navigation.} Google Maps \citep{googlemaps2023, google_gemini_maps_2025, velastegui2023google, schneider2022navigation} provides AI-powered navigation serving billions of users globally. Street-level AI \citep{froehlich2025streetreaderai, zamir2010accurate, torii2009google} enables detailed urban understanding. Navigation systems \citep{schneider2022navigation, malarvizhi2016use} integrate multiple data sources. \textbf{World Labs} \citep{worldlabs_2024} develops spatial intelligence for 3D world understanding, founded by Fei-Fei Li to advance AI systems that perceive and reason about physical space.

\subsection{Autonomous Vehicles}

\textbf{Waymo} \citep{waymo2023, waymo_emma_2024, waymo2024safety, sun2020waymoopen, sun2020scalability} has deployed autonomous vehicles at scale with millions of miles driven. EMMA \citep{waymo_emma_2024} provides end-to-end multimodal models for driving. Waymax \citep{gulino2023waymax} offers simulation for autonomous driving research. End-to-end approaches including UniAD \citep{hu2023uniad}, VAD \citep{jiang2023vad, jiang2024far3d}, DriveVLM \citep{tian2024drivevlm}, and S4Driver \citep{xie2025s4driver} unify perception, prediction, and planning.

\textbf{Tesla} \citep{tesla2023fsd, tesla2024autopilot, tesla_ai_2026} pursues vision-only autonomy with neural network-based planning. \textbf{Cruise} \citep{cruise2023}, \textbf{Mobileye} \citep{mobileye2023}, and \textbf{NVIDIA} \citep{nvidia_drive_2023} provide additional autonomous driving solutions. Motion prediction \citep{seff2023motionlm, mu2024most, teng2023motion, chen2024maptracker, chen2024vadv2, chen2024mapgpt2, liao2024maptrv2, li2022hdmapnet, liu2023vectormapnet} and perception \citep{zhang2023perception, antcliffe2019hd} are critical components.

\textbf{Datasets and Benchmarks.} nuScenes \citep{caesar2020nuscenes}, Waymo Open \citep{sun2020waymoopen, ettinger2021womd}, KITTI \citep{geiger2012kitti, behley2019semantickitti}, Argoverse \citep{argoverse2019, argoverse2}, CARLA \citep{dosovitskiy2017carla}, and EuRoC \citep{burri2016euroc} provide evaluation platforms.

\subsection{Robotics}

\textbf{Robot Learning Platforms.} Open X-Embodiment \citep{open_x_embodiment_rt_x_2023, padalkar2023openx, fang2023rh20t} provides large-scale robot data from Google DeepMind and collaborating institutions. Bridge Data \citep{walke2023bridgedata, ebert2021bridge, hussein2017imitation} enables cross-domain transfer. Simulation platforms \citep{james2020rlbench, yu2020metaworld, makoviychuk2021isaac, baruch2021arkitscenes, arunachalam2023holo, ramakrishnan2021hm3d} support policy development. Objaverse \citep{deitke2023objaverse, deitke2024objaversexl}, ShapeNet \citep{chang2015shapenet}, and ScanNet \citep{dai2017scannet, chang2017matterport3d} provide 3D assets. RH20T \citep{fang2023rh20t} provides large-scale robot manipulation data.

\textbf{Foundation Model Labs.} \textbf{OpenAI} \citep{openai2023gpt4, openai2023gpt4v, openai2024sora, openai2024gpt4o} advances multimodal AI with GPT-4V for vision-language understanding and Sora for video generation as world simulation. \textbf{Anthropic} \citep{anthropic2024claude, bai2022constitutional} develops Claude with constitutional AI principles for safe and helpful assistants. \textbf{Google DeepMind} \citep{team2023gemini, google2024gemini, brohan2022rt1, brohan2023rt2} leads in both foundation models (Gemini) and robotics (RT-1, RT-2). \textbf{Meta AI} \citep{touvron2023llama, touvron2023llama2, dubey2024llama3, oquab2023dinov2} provides open-source LLMs and vision models including DINOv2 for self-supervised visual learning.

\textbf{Emerging Applications.} Leidos C2AI \citep{leidos_c2ai_2025} applies agentic AI for command and control. Risk assessment \citep{rahman2025risk} uses spatial AI for safety. Site understanding \citep{wang2025site} enables construction applications.

\section{Evaluation Framework: SpatialAgentBench}

To address the lack of a unified evaluation standard, we propose \textbf{SpatialAgentBench}, a comprehensive suite of 8 tasks spanning all four spatial domains.

\begin{table}[h!]
\centering
\caption{Comparison of Spatial Intelligence Benchmarks}
\label{tab:benchmarks}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Benchmark} & \textbf{Task} & \textbf{Environment} & \textbf{Metrics} & \textbf{Key Feature} \\
\midrule
\multicolumn{5}{c}{\textbf{Navigation}} \\
\midrule
R2R \citep{anderson2018vln} & VLN & Real-world images & SPL, SR & First large-scale VLN \\
RxR \citep{ku2020room} & VLN & Real-world images & nDTW, SR & Multilingual \\
REVERIE \citep{qi2020reverie} & VLN & Real-world images & RGS & Remote grounding \\
Habitat ObjectNav \citep{batra2020objectnav} & ObjectNav & Simulated & SPL, Success & Standardized \\
SOON \citep{zhu2021soon} & ObjectNav & Simulated & NDO & Semantic \\
TouchDown \citep{chen2019touchdown} & VLN & Street View & TC, SPD & Urban navigation \\
EmbodiedBench \citep{yang2025embodiedbench} & Embodied & Simulated & Success Rate & Comprehensive \\
EmbodiedEval \citep{cheng2025embodiedeval} & Embodied & Simulated & Multiple & Multi-task \\
\midrule
\multicolumn{5}{c}{\textbf{Manipulation}} \\
\midrule
RLBench \citep{james2020rlbench} & 100+ tasks & Simulated & Success Rate & Diverse tasks \\
Meta-World \citep{yu2020metaworld} & 50 tasks & Simulated & Success Rate & Meta-learning \\
BEHAVIOR \citep{srivastava2021behavior} & 1000 activities & Simulated & Goal Conditions & Long-horizon \\
Open X-Embodiment \citep{open_x_embodiment_rt_x_2023} & 22 robots & Real-world & N/A & Largest real dataset \\
ManiSkill2 \citep{gu2023maniskill2} & 20 tasks & Simulated & Success Rate & Soft-body physics \\
CALVIN \citep{mees2022calvin} & Language & Simulated & Success Rate & Long-horizon language \\
VIMA \citep{sharma2022vima} & Multimodal & Simulated & Success Rate & Multimodal prompts \\
\midrule
\multicolumn{5}{c}{\textbf{Spatial Reasoning}} \\
\midrule
CLEVR \citep{johnson2017clevr} & VQA & Synthetic & Accuracy & Compositional \\
GQA \citep{hudson2019gqa} & VQA & Real-world & Accuracy & Scene graphs \\
SpatialVLM \citep{chen2024spatialvlm} & VQA & Real-world & Accuracy & Fine-grained spatial \\
ScanQA \citep{azuma2022scanqa} & 3D VQA & Real scans & EM, BLEU & 3D understanding \\
Open3D-VQA \citep{zhang2025open3dvqa} & 3D VQA & Real scans & Accuracy & Open-vocabulary \\
\midrule
\multicolumn{5}{c}{\textbf{Geospatial}} \\
\midrule
BigEarthNet \citep{sumbul2019bigearthnet} & Classification & Satellite & Accuracy, F1 & Large-scale \\
fMoW \citep{christie2018fmow} & Classification & Satellite & Accuracy & Temporal \\
xBD \citep{gupta2019xbd} & Segmentation & Satellite & IoU, F1 & Damage assessment \\
SpaceNet \citep{van2018spacenet} & Detection & Satellite & AP & Building footprints \\
GeoBench \citep{lacoste2024geobench} & Multiple & Satellite & Multiple & Standardized \\
\midrule
\multicolumn{5}{c}{\textbf{Agent Benchmarks}} \\
\midrule
AgentBench \citep{liu2023agentbench, ji2024llmbased, liu2024visualagentbench} & Multiple & Multiple & Success Rate & Comprehensive \\
WebArena \citep{zhou2023webarena} & Web & Browser & Success Rate & Web tasks \\
ALFWorld \citep{yao2021alfworld} & Embodied & Simulated & Success Rate & Text-world \\
SafeAgentBench \citep{safeagentbench2025} & Safety & Simulated & Safety Rate & Safety evaluation \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{SpatialAgentBench Tasks}

Our proposed benchmark includes eight tasks designed to evaluate the full spectrum of spatial agent capabilities:

\begin{enumerate}
    \item \textbf{VLN-Instruct}: Vision-language navigation with complex, multi-step instructions requiring spatial reasoning and landmark recognition.
    \item \textbf{ObjectSearch}: Multi-room object search with semantic reasoning, requiring agents to leverage commonsense knowledge about object locations.
    \item \textbf{SceneQA}: 3D scene question answering requiring understanding of spatial relationships, object properties, and scene semantics.
    \item \textbf{ManipSeq}: Sequential manipulation planning with long-horizon tasks requiring tool use and state tracking.
    \item \textbf{GeoReason}: Geospatial reasoning from satellite imagery including change detection, land use classification, and spatial pattern analysis.
    \item \textbf{TrafficPredict}: Spatio-temporal traffic prediction requiring modeling of complex urban dynamics and graph-structured dependencies.
    \item \textbf{SafeNav}: Navigation with safety constraints including obstacle avoidance, social navigation, and risk-aware planning.
    \item \textbf{MultiAgent}: Coordinated multi-agent spatial tasks requiring communication, task allocation, and collaborative planning.
\end{enumerate}

\subsection{Evaluation Metrics}

We propose standardized metrics across domains with formal definitions:

\textbf{Navigation Metrics.} Success Rate (SR) measures task completion. Success weighted by Path Length (SPL) \citep{anderson2018evaluation} accounts for path efficiency:
\begin{equation}
\text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{\ell_i}{\max(\ell_i, p_i)}
\end{equation}
where $S_i$ is the binary success indicator, $\ell_i$ is the shortest path length, and $p_i$ is the actual path length.

Normalized Dynamic Time Warping (nDTW) measures trajectory similarity:
\begin{equation}
\text{nDTW} = \exp\left(-\frac{\text{DTW}(P, R)}{\ell_R}\right)
\end{equation}
where $P$ is the predicted path, $R$ is the reference path, and $\ell_R$ is the reference path length.

\textbf{Manipulation Metrics.} Task Success Rate measures goal achievement. Goal Condition Satisfaction evaluates partial completion. Efficiency metrics include action count and time to completion.

\textbf{Reasoning Metrics.} Accuracy, F1 Score, and BLEU scores assess spatial reasoning and question answering.

\textbf{Safety Metrics.} Collision Rate, Safety Violation Rate, and Risk-Aware Success measure safe operation.

\subsection{Benchmark Limitations and Critical Analysis}

While existing benchmarks have advanced the field, several fundamental limitations warrant critical examination:

\textbf{Simulation-Reality Gap.} Most benchmarks rely on simulated environments \citep{savva2019habitat, kolve2017ai2thor, james2020rlbench, chattopadhyay2021robustnav}, which differ from real-world conditions in visual appearance, physics, and dynamics. Policies trained in simulation often fail to transfer \citep{zhao2020sim, tobin2017domain}, limiting practical applicability.

\textbf{Metric Limitations.} Standard metrics like SPL assume optimal paths are known, which is unrealistic in novel environments. Success Rate ignores partial progress and efficiency. Current metrics do not capture important aspects such as safety, robustness to perturbations, and graceful degradation.

\textbf{Dataset Biases.} Benchmarks often exhibit biases in object distributions, scene layouts, and instruction patterns \citep{anderson2018vln, ku2020room, cartillier2021semantic}. Models may exploit spurious correlations rather than learning generalizable spatial reasoning.

\textbf{Evaluation Scope.} Most benchmarks evaluate single capabilities in isolation. Real-world deployment requires integrated evaluation of perception, reasoning, planning, and action under uncertainty.

\textbf{Reproducibility Challenges.} Variations in simulation versions, random seeds, and evaluation protocols make cross-paper comparisons difficult. Standardized evaluation frameworks are needed.

\section{Open Challenges and Future Directions}

\subsection{Robust Spatial Representation}

Developing representations that generalize across scenes, viewpoints, and conditions remains challenging \citep{mildenhall2020nerf, kerbl20233dgaussian, barron2022mipnerf360, turki2022meganerf, luiten2023dynamic, lipson2021raft3d}. Foundation models for 3D understanding \citep{hong20233dllm, fu20243dfm, shen2023point, oquab2024dinov2, oquab2023dinov2, chen2023open, zhou2024uni3d, wu20153d} represent promising directions. Key challenges include:

\begin{itemize}
    \item \textbf{Occlusion handling}: Reasoning about hidden objects and occluded regions
    \item \textbf{Dynamic scenes}: Modeling temporal changes and object motion
    \item \textbf{Novel categories}: Generalizing to unseen object types and environments
    \item \textbf{Scale variation}: Handling objects and scenes across different scales
    \item \textbf{Viewpoint invariance}: Maintaining consistent understanding across perspectives
\end{itemize}

\subsection{Long-Horizon Planning}

Creating agents that plan over extended horizons and decompose complex spatial tasks is essential \citep{song2023llmplanner, valmeekam2023large, huang2022inner, li2025hiplanhierarchicalplanningllmbased, silver2024generalized, liu2023llm+, liu2023llmp, chen2022nlmapsaycan, chen2021waypoints, lavalle2006planning, hu2023planning}. Integration of neural and symbolic planning approaches \citep{garrett2021integrated, dantam2016incremental, li2020hybrid, ghallab2004automated, chen2019behavioral, tian2024graphcot, zhang2024proagent} shows promise. Challenges include:

\begin{itemize}
    \item \textbf{Credit assignment}: Attributing success or failure to specific actions
    \item \textbf{Subgoal discovery}: Automatically identifying useful intermediate goals
    \item \textbf{Plan repair}: Adapting plans when execution deviates from expectations
    \item \textbf{Temporal abstraction}: Operating at multiple time scales
    \item \textbf{Uncertainty handling}: Planning under incomplete information
\end{itemize}

\subsection{Safe and Reliable Operation}

Ensuring safe operation in safety-critical applications is paramount \citep{safeagentbench2025, amodei2016safety, amodei2016concrete, bai2022constitutional, ganguli2022red, perez2022red, hendrycks2021unsolved, hendrycks2021ethics, christiano2017rlhf, clark2019adversarial, itkina2022uncertainty}. Key requirements include:

\begin{itemize}
    \item \textbf{Uncertainty quantification}: Knowing when the agent is uncertain
    \item \textbf{Out-of-distribution detection}: Recognizing novel situations
    \item \textbf{Alignment}: Ensuring behavior matches human values and intentions
    \item \textbf{Interpretability}: Providing explanations for decisions
    \item \textbf{Graceful degradation}: Failing safely under adversarial conditions
    \item \textbf{Robustness}: Maintaining performance under distribution shift
\end{itemize}

\subsection{Sim-to-Real Transfer}

Bridging simulation and reality remains challenging \citep{zhao2020sim, tobin2017domain, james2019sim, matas2018sim, kaushik2020fast, chen2022structure, chen2022overlapnet, hofer2021sim2real}. The reality gap affects perception, dynamics, and control. Key approaches include:

\begin{itemize}
    \item \textbf{Domain randomization}: Training with varied simulation parameters
    \item \textbf{System identification}: Learning accurate dynamics models
    \item \textbf{Real-world fine-tuning}: Adapting with limited real data
    \item \textbf{Photorealistic simulation}: Reducing visual domain gap
    \item \textbf{Hybrid approaches}: Combining simulation and real-world data
\end{itemize}

\subsection{Multi-Agent Coordination}

Scaling to multi-agent systems for complex spatial tasks requires advances in coordination and communication \citep{zhang2021multi, wu2023autogen, hong2023metagpt, li2023s, qian2023communicative, yuan2023surveyprogresscooperativemultiagent, dang2025multiagentcollaborationevolvingorchestration, chen2024maagent, chen2023deep, ke2026masorchestraunderstandingimprovingmultiagent, wu2020graph_social}. Challenges include:

\begin{itemize}
    \item \textbf{Emergent communication}: Developing shared protocols
    \item \textbf{Credit assignment}: Attributing team success to individuals
    \item \textbf{Scalable coordination}: Handling large numbers of agents
    \item \textbf{Heterogeneous teams}: Coordinating diverse agent types
    \item \textbf{Partial observability}: Operating with limited information
\end{itemize}

\subsection{Efficiency and Deployment}

Deploying spatial AI systems on resource-constrained platforms requires advances in model compression, efficient inference, and edge computing \citep{han2016deep, howard2017mobilenets, dehghani2023scaling, fujita2022large, shen2023distilled}. Considerations include:

\begin{itemize}
    \item \textbf{Model compression}: Reducing model size while maintaining performance
    \item \textbf{Efficient architectures}: Designing compute-efficient models
    \item \textbf{Hardware acceleration}: Leveraging specialized hardware
    \item \textbf{Real-time operation}: Meeting latency requirements
    \item \textbf{Energy efficiency}: Operating within power constraints
\end{itemize}

\subsection{Emerging Directions}

Several emerging directions show promise for advancing autonomous spatial intelligence:

\textbf{Thinking Models.} Models that explicitly reason about spatial relationships \citep{yang2025thinking, chen2022think, Yang2025, ouyang2022instructgpt, ouyang2022rlhf} may improve planning and decision-making.

\textbf{Autonomous GIS.} Autonomous geographic information systems \citep{li2025autonomousgis, li2024geospatial, ma2025geographically, roberts2023gpt4geo} integrate agentic capabilities with geospatial analysis.

\textbf{Agentic AI for Specialized Domains.} Applications in defense \citep{nguyen2026agentic, abid2025ai}, disaster response \citep{elshenety_agentic_ai_sar}, and risk assessment \citep{rahman2025risk} represent growing areas. BIM integration \citep{bikandi2025bim}, 6G networks \citep{chauhan20266g}, and urban habitat \citep{UNHabitat2025} extend spatial AI applications.

\textbf{Multimodal Integration.} Deeper integration of vision, language, audio, and tactile modalities \citep{almadhor2026multimodal, yue2024mmmu, gao2024sphinxx, bar2024navigation} enables richer environmental understanding. SpatialBot \citep{cai2024spatialbot} and InternLM \citep{dong2024internlm} advance multimodal spatial reasoning.

\section{Limitations}

This survey, while comprehensive, has several limitations:

\begin{itemize}
    \item Our paper selection process, though systematic, may have missed relevant works in adjacent fields or non-English publications.
    \item The proposed taxonomy, while unifying, is one of many possible categorizations and may not capture all nuances of the field.
    \item Our analysis is based on publicly available information and does not include proprietary details from industry labs.
    \item The field is rapidly evolving, and some recent works may not be fully represented.
    \item We focus primarily on English-language publications from major venues.
    \item The proposed SpatialAgentBench is conceptual and requires implementation and validation.
    \item Our analysis of industry applications relies on public information and may not reflect current capabilities.
\end{itemize}

\section{Conclusion}

This survey has provided a unified taxonomy connecting Agentic AI and Spatial Intelligence, synthesizing over 900 papers across foundational architectures, state-of-the-art methods, industry applications, and evaluation benchmarks. Our analysis reveals three key findings:

\begin{enumerate}
    \item \textbf{Hierarchical memory systems} are critical for long-horizon spatial tasks, enabling agents to accumulate and retrieve spatial knowledge effectively. Advances in retrieval-augmented generation, episodic memory, and spatial memory representations provide foundations for persistent spatial understanding.
    
    \item \textbf{GNN-LLM integration} is an emergent paradigm combining the relational reasoning of graph networks with the semantic understanding of language models. This integration enables structured spatial reasoning that leverages both geometric relationships and semantic knowledge.
    
    \item \textbf{World models} are essential for safe deployment, enabling agents to predict consequences and plan in imagination before acting. Video world models, latent dynamics models, and LLM-based world models provide complementary approaches to predictive understanding.
\end{enumerate}

Key challenges remain in robust representation, long-horizon planning, safe deployment, sim-to-real transfer, multi-agent coordination, and efficient deployment. The convergence of vision-language-action models, graph neural networks, world models, and foundation models provides promising directions for addressing these challenges.

By establishing this foundational reference and proposing SpatialAgentBench, we aim to accelerate progress toward capable, robust, and safe spatially-aware autonomous systems that can perceive, reason about, and act within the physical world. The intersection of agentic AI and spatial intelligence represents a critical frontier for artificial intelligence, with profound implications for autonomous vehicles, robotics, urban computing, and geospatial intelligence.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
